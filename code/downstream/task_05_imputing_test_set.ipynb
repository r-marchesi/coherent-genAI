{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "\n",
    "# Data loading function remains the same\n",
    "def load_and_prepare_pancancer_data(labels_dir: str, data_dir: str):\n",
    "    # This function is unchanged from the previous version...\n",
    "    print(\"--- Loading and preparing all pan-cancer data... ---\")\n",
    "    X_train_orig = pd.read_csv(os.path.join(data_dir, \"real_data_train.csv\"), index_col=0)\n",
    "    X_test_orig = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"), index_col=0)\n",
    "    train_stage_df = pd.read_csv(os.path.join(labels_dir, \"train_stage.csv\"), index_col=0)\n",
    "    train_type_df = pd.read_csv(os.path.join(labels_dir, \"train_cancer_type.csv\"), index_col=0)\n",
    "    test_stage_df = pd.read_csv(os.path.join(labels_dir, \"test_stage.csv\"), index_col=0)\n",
    "    test_type_df = pd.read_csv(os.path.join(labels_dir, \"test_cancer_type.csv\"), index_col=0)\n",
    "    train_labels_combined = train_stage_df.join(train_type_df).dropna(subset=['stage', 'cancertype'])\n",
    "    test_labels_combined = test_stage_df.join(test_type_df).dropna(subset=['stage', 'cancertype'])\n",
    "    train_common_idx = train_labels_combined.index.intersection(X_train_orig.index)\n",
    "    test_common_idx = test_labels_combined.index.intersection(X_test_orig.index)\n",
    "    X_train = X_train_orig.loc[train_common_idx].sort_index()\n",
    "    y_train = train_labels_combined.loc[train_common_idx, 'stage'].sort_index()\n",
    "    train_types = train_labels_combined.loc[train_common_idx, 'cancertype'].sort_index()\n",
    "    X_test = X_test_orig.loc[test_common_idx].sort_index()\n",
    "    y_test = test_labels_combined.loc[test_common_idx, 'stage'].sort_index()\n",
    "    test_types = test_labels_combined.loc[test_common_idx, 'cancertype'].sort_index()\n",
    "    print(f\"  Found {len(X_train)} training samples and {len(X_test)} test samples across all cancers.\")\n",
    "    train_cancer_dummies = pd.get_dummies(train_types, prefix='cancer')\n",
    "    test_cancer_dummies = pd.get_dummies(test_types, prefix='cancer')\n",
    "    train_cancer_dummies, test_cancer_dummies = train_cancer_dummies.align(test_cancer_dummies, join='outer', axis=1, fill_value=0)\n",
    "    X_train_final = pd.concat([X_train, train_cancer_dummies], axis=1)\n",
    "    X_test_final = pd.concat([X_test, test_cancer_dummies], axis=1)\n",
    "    return X_train_final, y_train, X_test_final, y_test\n",
    "\n",
    "\n",
    "# MODIFIED FUNCTION - Now accepts a random_seed\n",
    "def run_pancancer_modality_analysis(labels_dir: str, data_dir: str, random_seed: int):\n",
    "    \"\"\"\n",
    "    Trains a single pan-cancer model with a specific random seed and evaluates\n",
    "    its robustness to combinations of missing modalities.\n",
    "    \"\"\"\n",
    "    # Data loading is now separated\n",
    "    X_train, y_train, X_test, y_test = load_and_prepare_pancancer_data(labels_dir, data_dir)\n",
    "    \n",
    "    if X_train is None: # Check if data loading failed\n",
    "        return None\n",
    "\n",
    "    # Train a SINGLE pan-cancer model with the given seed\n",
    "    print(f\"  Training a single pan-cancer model with random_state={random_seed}...\")\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=random_seed, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Dynamically find which modalities are present in the data\n",
    "    all_prefixes = {col.split('_')[0] for col in X_test.columns}\n",
    "    possible_modalities = ['cna', 'rnaseq', 'rppa', 'wsi'] \n",
    "    available_modalities = sorted([m for m in possible_modalities if m in all_prefixes])\n",
    "    \n",
    "    test_conditions = {'full_data': X_test}\n",
    "\n",
    "    # Generate all combinations of modalities to remove\n",
    "    for r in range(1, len(available_modalities) + 1): # Go up to ALL modalities removed\n",
    "        modality_combinations_to_remove = combinations(available_modalities, r)\n",
    "        \n",
    "        for combo in modality_combinations_to_remove:\n",
    "            condition_name = f\"no_{'_'.join(combo)}\"\n",
    "            cols_to_nullify = []\n",
    "            for modality in combo:\n",
    "                cols_to_nullify.extend([col for col in X_test.columns if col.startswith(modality + '_')])\n",
    "            \n",
    "            X_test_modified = X_test.copy()\n",
    "            X_test_modified[cols_to_nullify] = np.nan\n",
    "            test_conditions[condition_name] = X_test_modified\n",
    "            \n",
    "    # Evaluate the single model on each test condition\n",
    "    results = []\n",
    "    for condition_name, X_test_current in test_conditions.items():\n",
    "        y_pred = model.predict(X_test_current)\n",
    "        balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "        macro_f1 = classification_report(y_test, y_pred, output_dict=True, zero_division=0)['macro avg']['f1-score']\n",
    "        results.append({\n",
    "            'test_condition': condition_name,\n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'macro_f1_score': macro_f1\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    LABELS_DIR = \"../../datasets_TCGA/downstream_labels\"\n",
    "    DATA_DIR = \"./data_task_02\"\n",
    "    N_RUNS = 10  # Define how many times to run the experiment\n",
    "    \n",
    "    all_results_dfs = []\n",
    "    \n",
    "    # --- Main Repetition Loop ---\n",
    "    for i in range(N_RUNS):\n",
    "        print(f\"\\n==================== Starting Run {i+1}/{N_RUNS} ====================\")\n",
    "        # Pass 'i' as the random seed for reproducibility and variation\n",
    "        results_df = run_pancancer_modality_analysis(\n",
    "            labels_dir=LABELS_DIR,\n",
    "            data_dir=DATA_DIR,\n",
    "            random_seed=i \n",
    "        )\n",
    "        \n",
    "        if results_df is not None:\n",
    "            results_df['run'] = i + 1 # Add a column to identify the run\n",
    "            all_results_dfs.append(results_df)\n",
    "\n",
    "    # --- Aggregate and Analyze Final Results ---\n",
    "    if all_results_dfs:\n",
    "        # Combine results from all runs into a single DataFrame\n",
    "        final_results_all_runs = pd.concat(all_results_dfs, ignore_index=True)\n",
    "\n",
    "        print(\"\\n\\n===== SUMMARY STATISTICS ACROSS ALL RUNS =====\")\n",
    "        # Calculate summary statistics for each test condition\n",
    "        summary_stats = final_results_all_runs.groupby('test_condition')['balanced_accuracy'].agg(\n",
    "            ['mean', 'std', 'median', 'min', 'max']\n",
    "        ).sort_values(by='mean', ascending=False)\n",
    "        print(summary_stats)\n",
    "\n",
    "        # --- MODIFIED VISUALIZATION BLOCK ---\n",
    "        print(\"\\n\\n--- Generating plot to visualize performance and variance ---\")\n",
    "\n",
    "        # 1. Define a logical order for the plot\n",
    "        summary_stats['n_removed'] = summary_stats.index.str.count('_')\n",
    "        plot_order = summary_stats.sort_values(by=['n_removed', 'mean'], ascending=[True, False]).index.tolist()\n",
    "\n",
    "        # 2. Create the box plot to show the distribution of results\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        ax = sns.boxplot(\n",
    "            data=final_results_all_runs,\n",
    "            x='test_condition',\n",
    "            y='balanced_accuracy',\n",
    "            order=plot_order,\n",
    "            showfliers=False # Hide outlier points for clarity\n",
    "        )\n",
    "        # Overlay a stripplot to show individual data points from each run\n",
    "        sns.stripplot(\n",
    "            data=final_results_all_runs,\n",
    "            x='test_condition',\n",
    "            y='balanced_accuracy',\n",
    "            order=plot_order,\n",
    "            jitter=True,\n",
    "            alpha=0.6,\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "        # 3. Final plot formatting\n",
    "        ax.set_title('Pan-Cancer Model Performance and Stability Across Multiple Runs', fontsize=16)\n",
    "        ax.set_xlabel('Test Data Condition', fontsize=12)\n",
    "        ax.set_ylabel('Balanced Accuracy', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "import torch\n",
    "import json\n",
    "import pathlib\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- Add your custom library path ---\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) \n",
    "from lib.test import coherent_test_cos_rejection, test_model\n",
    "from lib.config import modalities_list\n",
    "from lib.get_models import get_diffusion_model\n",
    "from lib.diffusion_models import GaussianDiffusion\n",
    "\n",
    "# --- SECTION 1: HELPER FUNCTIONS (UNCHANGED) ---\n",
    "def load_and_prepare_pancancer_data(labels_dir: str, data_dir: str):\n",
    "    # This function is unchanged from the previous version...\n",
    "    print(\"--- Loading and preparing all pan-cancer data... ---\")\n",
    "    X_train_orig = pd.read_csv(os.path.join(data_dir, \"real_data_train.csv\"), index_col=0)\n",
    "    X_test_orig = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"), index_col=0)\n",
    "    train_stage_df = pd.read_csv(os.path.join(labels_dir, \"train_stage.csv\"), index_col=0)\n",
    "    train_type_df = pd.read_csv(os.path.join(labels_dir, \"train_cancer_type.csv\"), index_col=0)\n",
    "    test_stage_df = pd.read_csv(os.path.join(labels_dir, \"test_stage.csv\"), index_col=0)\n",
    "    test_type_df = pd.read_csv(os.path.join(labels_dir, \"test_cancer_type.csv\"), index_col=0)\n",
    "    train_labels_combined = train_stage_df.join(train_type_df).dropna(subset=['stage', 'cancertype'])\n",
    "    test_labels_combined = test_stage_df.join(test_type_df).dropna(subset=['stage', 'cancertype'])\n",
    "    train_common_idx = train_labels_combined.index.intersection(X_train_orig.index)\n",
    "    test_common_idx = test_labels_combined.index.intersection(X_test_orig.index)\n",
    "    X_train = X_train_orig.loc[train_common_idx].sort_index()\n",
    "    y_train = train_labels_combined.loc[train_common_idx, 'stage'].sort_index()\n",
    "    train_types = train_labels_combined.loc[train_common_idx, 'cancertype'].sort_index()\n",
    "    X_test = X_test_orig.loc[test_common_idx].sort_index()\n",
    "    y_test = test_labels_combined.loc[test_common_idx, 'stage'].sort_index()\n",
    "    test_types = test_labels_combined.loc[test_common_idx, 'cancertype'].sort_index()\n",
    "    print(f\"  Found {len(X_train)} training samples and {len(X_test)} test samples across all cancers.\")\n",
    "    train_cancer_dummies = pd.get_dummies(train_types, prefix='cancer')\n",
    "    test_cancer_dummies = pd.get_dummies(test_types, prefix='cancer')\n",
    "    train_cancer_dummies, test_cancer_dummies = train_cancer_dummies.align(test_cancer_dummies, join='outer', axis=1, fill_value=0)\n",
    "    X_train_final = pd.concat([X_train, train_cancer_dummies], axis=1)\n",
    "    X_test_final = pd.concat([X_test, test_cancer_dummies], axis=1)\n",
    "    return X_train_final, y_train, X_test_final, y_test\n",
    "\n",
    "def load_single_model(target_mod, cond_mod, diffusion, config_args, device):\n",
    "    # This function is unchanged from the previous version...\n",
    "    path = pathlib.Path(f'../../{config_args.folder}/{config_args.dim}/{target_mod}_from_{cond_mod}')\n",
    "    ckpt_path = path / f'train/best_by_{config_args.metric}.pth'\n",
    "    if not ckpt_path.exists(): raise FileNotFoundError(f\"Checkpoint not found for single model: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    config = SimpleNamespace(**ckpt[\"config\"])\n",
    "    x_dim = config_args.modality_dims[target_mod]\n",
    "    cond_dim = config_args.modality_dims[cond_mod]\n",
    "    model = get_diffusion_model(config.architecture, diffusion, config, x_dim=x_dim, cond_dims=cond_dim).to(device)\n",
    "    model.load_state_dict(ckpt[f\"best_model_{config_args.metric}\"])\n",
    "    model.eval()\n",
    "    return model, config, ckpt['best_loss']\n",
    "\n",
    "def load_multi_model(target_mod, diffusion, config_args, device):\n",
    "    # This function is unchanged from the previous version...\n",
    "    base_dir = pathlib.Path(f\"../../{config_args.folder}/{config_args.dim}/{target_mod}_from_multi{'_masked' if config_args.mask else ''}\")\n",
    "    ckpt_path = base_dir / 'train' / f'best_by_{config_args.metric}.pth'\n",
    "    if not ckpt_path.exists(): raise FileNotFoundError(f\"Checkpoint not found for multi model: {ckpt_path}\")\n",
    "    with open(base_dir / 'cond_order.json', 'r') as f: cond_order = json.load(f)\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "    config = SimpleNamespace(**ckpt['config'])\n",
    "    x_dim = config_args.modality_dims[target_mod]\n",
    "    cond_dim_list = [config_args.modality_dims[c] for c in cond_order]\n",
    "    model = get_diffusion_model(config.architecture, diffusion, config, x_dim=x_dim, cond_dims=cond_dim_list).to(device)\n",
    "    model.load_state_dict(ckpt[f'best_model_{config_args.metric}'])\n",
    "    model.eval()\n",
    "    return model, config, cond_order\n",
    "\n",
    "def impute_missing_modalities(X_test_with_nan, modalities_to_impute, available_modalities, gen_mode, config_args, diffusion, device):\n",
    "    # This function is unchanged from the previous version...\n",
    "    X_imputed = X_test_with_nan.copy()\n",
    "    generation_order = sorted(modalities_to_impute)\n",
    "    conditioning_modalities = [m for m in available_modalities if m not in modalities_to_impute]\n",
    "    for i, target_mod in enumerate(generation_order):\n",
    "        print(f\"    Imputing '{target_mod}' (step {i+1}/{len(generation_order)}) with '{gen_mode}' model...\")\n",
    "        current_conds = conditioning_modalities + generation_order[:i]\n",
    "        cond_data_list = []\n",
    "        for cond_mod in current_conds:\n",
    "            cond_cols = [c for c in X_imputed.columns if c.startswith(cond_mod + '_')]\n",
    "            cond_data_list.append(X_imputed[cond_cols])\n",
    "        if gen_mode == 'coherent':\n",
    "            models = [load_single_model(target_mod, c, diffusion, config_args, device)[0] for c in current_conds]\n",
    "            weights = [load_single_model(target_mod, c, diffusion, config_args, device)[2] for c in current_conds]\n",
    "            _, generated_df, _ = coherent_test_cos_rejection(\n",
    "                pd.DataFrame(np.zeros((X_imputed.shape[0], config_args.modality_dims[target_mod]))), \n",
    "                cond_data_list, models, diffusion, test_iterations=1, max_retries=10, \n",
    "                device=device, weights_list=weights\n",
    "            )\n",
    "        elif gen_mode == 'multi':\n",
    "            model, _, cond_order = load_multi_model(target_mod, diffusion, config_args, device)\n",
    "            final_cond_list = []\n",
    "            for c_name in cond_order:\n",
    "                if c_name in current_conds:\n",
    "                    cond_cols = [c for c in X_imputed.columns if c.startswith(c_name + '_')]\n",
    "                    final_cond_list.append(X_imputed[cond_cols])\n",
    "                else: \n",
    "                    shape = (X_imputed.shape[0], config_args.modality_dims[c_name])\n",
    "                    final_cond_list.append(pd.DataFrame(np.zeros(shape)))\n",
    "            _, generated_df = test_model(\n",
    "                pd.DataFrame(np.zeros((X_imputed.shape[0], config_args.modality_dims[target_mod]))),\n",
    "                final_cond_list, model, diffusion, test_iterations=1, device=device\n",
    "            )\n",
    "        target_cols = [c for c in X_imputed.columns if c.startswith(target_mod + '_')]\n",
    "        generated_df.columns = target_cols\n",
    "        generated_df.index = X_imputed.index\n",
    "        X_imputed[target_cols] = generated_df\n",
    "    return X_imputed\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    \"\"\"Helper function to calculate and return all desired metrics.\"\"\"\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    macro_f1 = report['macro avg']['f1-score']\n",
    "    return balanced_acc, macro_f1\n",
    "\n",
    "# --- SECTION 2: MAIN ANALYSIS PIPELINE (MODIFIED) ---\n",
    "\n",
    "def run_pancancer_analysis_with_imputation(random_seed: int, config_args, diffusion, device):\n",
    "    X_train, y_train, X_test, y_test = load_and_prepare_pancancer_data(\n",
    "        config_args.labels_dir, config_args.data_dir\n",
    "    )\n",
    "    if X_train is None: return None\n",
    "\n",
    "    print(f\"\\n--- Training pan-cancer model with random_state={random_seed}... ---\")\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=random_seed, n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    all_prefixes = {col.split('_')[0] for col in X_test.columns if '_' in col}\n",
    "    possible_modalities = ['cna', 'rnaseq', 'rppa', 'wsi'] \n",
    "    available_modalities = sorted([m for m in possible_modalities if m in all_prefixes])\n",
    "    print(f\"  Available modalities for ablation/imputation: {available_modalities}\")\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    # --- NEW: Step 1 - Evaluate on the full, unmodified test set first ---\n",
    "    print(\"\\n--- Processing Test Condition: full_data ---\")\n",
    "    y_pred_full = classifier.predict(X_test)\n",
    "    b_acc, f1 = get_metrics(y_test, y_pred_full)\n",
    "    all_results.append({\n",
    "        'test_condition': 'full_data',\n",
    "        'test_type': 'full_data',\n",
    "        'balanced_accuracy': b_acc,\n",
    "        'macro_f1_score': f1\n",
    "    })\n",
    "\n",
    "    # --- NEW: Step 2 - Loop through combinations of modalities to remove/impute ---\n",
    "    for r in range(1, len(available_modalities) + 1):\n",
    "        for combo in combinations(available_modalities, r):\n",
    "            \n",
    "            # NEW: Rename the final condition for clarity\n",
    "            if len(combo) == len(available_modalities):\n",
    "                condition_name = \"cancer_label_only\"\n",
    "            else:\n",
    "                condition_name = f\"no_{'_'.join(combo)}\"\n",
    "                \n",
    "            modalities_to_process = list(combo)\n",
    "            print(f\"\\n--- Processing Test Condition: {condition_name} ---\")\n",
    "\n",
    "            X_test_ablated = X_test.copy()\n",
    "            cols_to_nullify = [col for mod in modalities_to_process for col in X_test.columns if col.startswith(mod + '_')]\n",
    "            X_test_ablated[cols_to_nullify] = np.nan\n",
    "            \n",
    "            y_pred_ablated = classifier.predict(X_test_ablated)\n",
    "            b_acc, f1 = get_metrics(y_test, y_pred_ablated)\n",
    "            all_results.append({\n",
    "                'test_condition': condition_name,\n",
    "                'test_type': 'ablation',\n",
    "                'balanced_accuracy': b_acc,\n",
    "                'macro_f1_score': f1\n",
    "            })\n",
    "\n",
    "            if len(modalities_to_process) < len(available_modalities):\n",
    "                for gen_mode in ['multi', 'coherent']:\n",
    "                    X_test_imputed = impute_missing_modalities(\n",
    "                        X_test_ablated, modalities_to_process, available_modalities, \n",
    "                        gen_mode, config_args, diffusion, device\n",
    "                    )\n",
    "                    y_pred_imputed = classifier.predict(X_test_imputed)\n",
    "                    b_acc, f1 = get_metrics(y_test, y_pred_imputed)\n",
    "                    all_results.append({\n",
    "                        'test_condition': condition_name,\n",
    "                        'test_type': f'imputed_{gen_mode}',\n",
    "                        'balanced_accuracy': b_acc,\n",
    "                        'macro_f1_score': f1\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"  Skipping generative imputation for {condition_name}.\")\n",
    "                for gen_mode in ['multi', 'coherent']:\n",
    "                    all_results.append({\n",
    "                        'test_condition': condition_name,\n",
    "                        'test_type': f'imputed_{gen_mode}',\n",
    "                        'balanced_accuracy': np.nan,\n",
    "                        'macro_f1_score': np.nan\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# --- SECTION 3: MAIN EXECUTION AND VISUALIZATION (MODIFIED) ---\n",
    "\n",
    "def create_summary_plot(data: pd.DataFrame, metric: str, title: str):\n",
    "    \"\"\"Helper function to create a consistent plot for a given metric.\"\"\"\n",
    "    print(f\"\\n--- Generating plot for: {metric} ---\")\n",
    "    \n",
    "    # Define a logical order for the plot\n",
    "    data['n_removed'] = data['test_condition'].apply(\n",
    "        lambda x: 0 if x == 'full_data' else x.count('_') + 1\n",
    "    )\n",
    "    # Special case for the final condition\n",
    "    data.loc[data['test_condition'] == 'cancer_label_only', 'n_removed'] = data['n_removed'].max() + 1\n",
    "    \n",
    "    plot_order = data.sort_values(by=['n_removed', 'test_condition']).test_condition.unique()\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=data, x='test_condition', y=metric, hue='test_type',\n",
    "        order=plot_order, kind='bar', height=6, aspect=2.5, legend_out=False, errorbar='sd'\n",
    "    )\n",
    "    g.fig.suptitle(title, y=1.03, fontsize=16)\n",
    "    g.set_axis_labels(\"Test Condition (Modalities Removed / Available)\", f\"Mean {metric.replace('_', ' ').title()}\")\n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    plt.legend(title='Test Type')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config_args = SimpleNamespace(\n",
    "        folder='results', metric='mse', dim='32', mask=False,\n",
    "        labels_dir=\"../../datasets_TCGA/downstream_labels\",\n",
    "        data_dir=\"./data_task_02\",\n",
    "        modality_dims={'cna': 32, 'rnaseq': 32, 'rppa': 32, 'wsi': 32}\n",
    "    )\n",
    "    device = torch.device(f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    diffusion = GaussianDiffusion(num_timesteps=1000).to(device)\n",
    "    N_RUNS = 5\n",
    "    all_run_dfs = []\n",
    "\n",
    "    for i in range(N_RUNS):\n",
    "        print(f\"\\n{'='*25} Starting Run {i+1}/{N_RUNS} {'='*25}\")\n",
    "        results_df = run_pancancer_analysis_with_imputation(\n",
    "            random_seed=i, config_args=config_args, diffusion=diffusion, device=device\n",
    "        )\n",
    "        if results_df is not None:\n",
    "            results_df['run'] = i + 1\n",
    "            all_run_dfs.append(results_df)\n",
    "\n",
    "    if all_run_dfs:\n",
    "        final_results = pd.concat(all_run_dfs, ignore_index=True)\n",
    "        final_results.dropna(subset=['balanced_accuracy', 'macro_f1_score'], inplace=True)\n",
    "\n",
    "        print(\"\\n\\n===== SUMMARY STATISTICS ACROSS ALL RUNS =====\")\n",
    "        summary_stats = final_results.groupby(['test_condition', 'test_type'])[['balanced_accuracy', 'macro_f1_score']].agg(\n",
    "            ['mean', 'std', 'median']\n",
    "        )\n",
    "        # Define a logical sort order for the summary table\n",
    "        summary_stats['n_removed'] = summary_stats.index.get_level_values('test_condition').map(\n",
    "            lambda x: 0 if x == 'full_data' else (x.count('_') + 2 if x == 'cancer_label_only' else x.count('_') + 1)\n",
    "        )\n",
    "        print(summary_stats.sort_values(by=['n_removed', ('balanced_accuracy', 'mean')], ascending=[True, False]).drop(columns='n_removed').to_string())\n",
    "\n",
    "        # Create a plot for each metric\n",
    "        create_summary_plot(final_results, 'balanced_accuracy', 'Comparison of Imputation Strategies (Balanced Accuracy)')\n",
    "        create_summary_plot(final_results, 'macro_f1_score', 'Comparison of Imputation Strategies (Macro F1-Score)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_plot(data: pd.DataFrame, metric: str, title: str, save_path = None):\n",
    "    \"\"\"Helper function to create a consistent plot for a given metric.\"\"\"\n",
    "    print(f\"\\n--- Generating plot for: {metric} ---\")\n",
    "    \n",
    "    # Define a logical order for the plot\n",
    "    data['n_removed'] = data['test_condition'].apply(\n",
    "        lambda x: 0 if x == 'full_data' else x.count('_') + 1\n",
    "    )\n",
    "    # Special case for the final condition\n",
    "    data.loc[data['test_condition'] == 'cancer_label_only', 'n_removed'] = data['n_removed'].max() + 1\n",
    "    \n",
    "    plot_order = data.sort_values(by=['n_removed', 'test_condition']).test_condition.unique()\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=data, x='test_condition', y=metric, hue='test_type',\n",
    "        order=plot_order, kind='bar', height=6, aspect=2.5, legend_out=True, errorbar='sd'\n",
    "    )\n",
    "    g.fig.suptitle(title, y=1.03, fontsize=16)\n",
    "    g.set_axis_labels(\"Test Condition (Modalities Removed)\", f\"Mean {metric.replace('_', ' ').title()}\")\n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    g._legend.set_title('Test Type')\n",
    "    g._legend.set_bbox_to_anchor((1.02, 0.5))\n",
    "    g._legend.set_frame_on(True)\n",
    "    g._legend.set_loc('center left')\n",
    "    if save_path is not None:\n",
    "        g.savefig(save_path)\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    plt.show()        \n",
    "\n",
    "\n",
    "final_results = pd.concat(all_run_dfs, ignore_index=True)\n",
    "final_results.dropna(subset=['balanced_accuracy', 'macro_f1_score'], inplace=True)\n",
    "\n",
    "print(\"\\n\\n===== SUMMARY STATISTICS ACROSS ALL RUNS =====\")\n",
    "summary_stats = final_results.groupby(['test_condition', 'test_type'])[['balanced_accuracy', 'macro_f1_score']].agg(['mean', 'std', 'median'])\n",
    "\n",
    "# Define a logical sort order for the summary table\n",
    "summary_stats['n_removed'] = summary_stats.index.get_level_values('test_condition').map(\n",
    "    lambda x: 0 if x == 'full_data' else (x.count('_') + 2 if x == 'cancer_label_only' else x.count('_') + 1)\n",
    ")\n",
    "print(summary_stats.sort_values(by=['n_removed', ('balanced_accuracy', 'mean')], ascending=[True, False]).drop(columns='n_removed').to_string())\n",
    "\n",
    "results_path = '../../results/downstream/task_05_imputing_test_set'\n",
    "# Ensure the results directory exists\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# Save the final results to a CSV file\n",
    "final_results.to_csv(os.path.join(results_path, 'results.csv'), index=False)\n",
    "\n",
    "\n",
    "# Create a plot for each metric\n",
    "create_summary_plot(final_results, 'balanced_accuracy', 'Comparison of Imputation Strategies (Balanced Accuracy)', save_path=os.path.join(results_path, 'balanced_accuracy_plot.png'))\n",
    "create_summary_plot(final_results, 'macro_f1_score', 'Comparison of Imputation Strategies (Macro F1-Score)', save_path=os.path.join(results_path, 'f1_score_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67402bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import numpy as np\n",
    "\n",
    "def analyze_significance_of_imputation(results_df: pd.DataFrame, metric: str = 'balanced_accuracy'):\n",
    "    \"\"\"\n",
    "    Performs a statistical analysis to find where imputation provides a significant gain\n",
    "    over the ablation (NaN) method.\n",
    "    \n",
    "    Args:\n",
    "        results_df: The DataFrame loaded from your results CSV file.\n",
    "        metric: The performance metric to analyze ('balanced_accuracy' or 'macro_f1_score').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the dataframe is not empty and has the required columns\n",
    "    if results_df.empty or not all(c in results_df.columns for c in ['test_condition', 'test_type', metric]):\n",
    "        print(\"DataFrame is empty or missing required columns.\")\n",
    "        return None\n",
    "\n",
    "    # This list will store the results of each statistical test\n",
    "    statistical_results = []\n",
    "    \n",
    "    # Get all unique test conditions, excluding the 'full_data' baseline\n",
    "    test_conditions = [c for c in results_df['test_condition'].unique() if c != 'full_data']\n",
    "\n",
    "    # --- Loop through each condition to perform comparisons ---\n",
    "    for condition in test_conditions:\n",
    "        condition_df = results_df[results_df['test_condition'] == condition]\n",
    "        \n",
    "        # Get the performance scores for each test type from all runs\n",
    "        try:\n",
    "            scores_ablation = condition_df[condition_df['test_type'] == 'ablation'][metric].dropna()\n",
    "            scores_multi = condition_df[condition_df['test_type'] == 'imputed_multi'][metric].dropna()\n",
    "            scores_coherent = condition_df[condition_df['test_type'] == 'imputed_coherent'][metric].dropna()\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Could not find all test types for condition '{condition}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Define the comparisons we want to make\n",
    "        comparisons = {\n",
    "            \"imputed_multi vs. ablation\": (scores_multi, scores_ablation),\n",
    "            \"imputed_coherent vs. ablation\": (scores_coherent, scores_ablation),\n",
    "        }\n",
    "\n",
    "        for comp_name, (scores_imputed, scores_base) in comparisons.items():\n",
    "            # Ensure we have paired data to compare\n",
    "            if len(scores_imputed) != len(scores_base) or len(scores_imputed) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the mean performance gain\n",
    "            mean_gain = scores_imputed.mean() - scores_base.mean()\n",
    "            \n",
    "            # Perform the one-sided Wilcoxon signed-rank test\n",
    "            # H1: The distribution of scores_imputed is greater than scores_base\n",
    "            try:\n",
    "                stat, p_value = wilcoxon(scores_imputed, scores_base, alternative='greater')\n",
    "            except ValueError:\n",
    "                # This can happen if all differences are zero\n",
    "                p_value = 1.0\n",
    "\n",
    "            statistical_results.append({\n",
    "                'test_condition': condition,\n",
    "                'comparison': comp_name,\n",
    "                'mean_gain': mean_gain,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "\n",
    "    if not statistical_results:\n",
    "        print(\"No valid comparisons could be made.\")\n",
    "        return None\n",
    "\n",
    "    # --- Apply Multiple Testing Correction ---\n",
    "    stats_df = pd.DataFrame(statistical_results)\n",
    "    \n",
    "    # Use the Benjamini-Hochberg FDR correction method\n",
    "    reject, p_adj, _, _ = multipletests(stats_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "    \n",
    "    stats_df['p_adj_fdr'] = p_adj\n",
    "    stats_df['significant_gain'] = reject # True if p_adj < 0.05\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "\n",
    "    # Load the results file you saved from the previous run\n",
    "    try:\n",
    "        results_file = '../../results/downstream/task_05_imputing_test_set/results.csv'\n",
    "        all_results = pd.read_csv(results_file)\n",
    "        print(f\"Successfully loaded results from '{results_file}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The results file '{results_file}' was not found.\")\n",
    "        print(\"Please run the previous script first to generate the results.\")\n",
    "        exit()\n",
    "\n",
    "    # Analyze the results based on Balanced Accuracy\n",
    "    print(\"\\n\\n===== Analyzing Significance for Balanced Accuracy =====\")\n",
    "    significance_df_accuracy = analyze_significance_of_imputation(all_results, metric='balanced_accuracy')\n",
    "\n",
    "    if significance_df_accuracy is not None:\n",
    "        # Sort the table to easily see the most significant gains\n",
    "        sorted_accuracy_results = significance_df_accuracy.sort_values(\n",
    "            by=['significant_gain', 'p_adj_fdr'], \n",
    "            ascending=[False, True]\n",
    "        )\n",
    "        print(\"The table below shows if an imputation method provided a statistically significant gain over ablation.\")\n",
    "        print(\"A 'significant_gain' of True means we can be confident the improvement was not due to random chance.\")\n",
    "        print(sorted_accuracy_results.to_string())\n",
    "    \n",
    "    # Analyze the results based on Macro F1-Score\n",
    "    print(\"\\n\\n===== Analyzing Significance for Macro F1-Score =====\")\n",
    "    significance_df_f1 = analyze_significance_of_imputation(all_results, metric='macro_f1_score')\n",
    "\n",
    "    if significance_df_f1 is not None:\n",
    "        sorted_f1_results = significance_df_f1.sort_values(\n",
    "            by=['significant_gain', 'p_adj_fdr'], \n",
    "            ascending=[False, True]\n",
    "        )\n",
    "        print(\"The table below shows if an imputation method provided a statistically significant gain over ablation.\")\n",
    "        print(\"A 'significant_gain' of True means we can be confident the improvement was not due to random chance.\")\n",
    "        print(sorted_f1_results.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aea39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel \n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import numpy as np\n",
    "\n",
    "def analyze_significance_with_ttest(results_df: pd.DataFrame, metric: str = 'balanced_accuracy'):\n",
    "    \"\"\"\n",
    "    Performs a statistical analysis using a paired t-test to find where imputation \n",
    "    provides a significant gain over the ablation (NaN) method.\n",
    "    \n",
    "    Args:\n",
    "        results_df: The DataFrame loaded from your results CSV file.\n",
    "        metric: The performance metric to analyze ('balanced_accuracy' or 'macro_f1_score').\n",
    "    \"\"\"\n",
    "    \n",
    "    if results_df.empty or not all(c in results_df.columns for c in ['test_condition', 'test_type', metric]):\n",
    "        print(\"DataFrame is empty or missing required columns.\")\n",
    "        return None\n",
    "\n",
    "    statistical_results = []\n",
    "    \n",
    "    test_conditions = [c for c in results_df['test_condition'].unique() if c != 'full_data']\n",
    "\n",
    "    # --- Loop through each condition to perform comparisons ---\n",
    "    for condition in test_conditions:\n",
    "        condition_df = results_df[results_df['test_condition'] == condition]\n",
    "        \n",
    "        try:\n",
    "            scores_ablation = condition_df[condition_df['test_type'] == 'ablation'][metric].dropna()\n",
    "            scores_multi = condition_df[condition_df['test_type'] == 'imputed_multi'][metric].dropna()\n",
    "            scores_coherent = condition_df[condition_df['test_type'] == 'imputed_coherent'][metric].dropna()\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Could not find all test types for condition '{condition}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        comparisons = {\n",
    "            \"imputed_multi vs. ablation\": (scores_multi, scores_ablation),\n",
    "            \"imputed_coherent vs. ablation\": (scores_coherent, scores_ablation),\n",
    "        }\n",
    "\n",
    "        for comp_name, (scores_imputed, scores_base) in comparisons.items():\n",
    "            if len(scores_imputed) != len(scores_base) or len(scores_imputed) < 2: # t-test needs at least 2 observations\n",
    "                continue\n",
    "\n",
    "            mean_gain = scores_imputed.mean() - scores_base.mean()\n",
    "            \n",
    "            # --- THIS IS THE MODIFIED PART ---\n",
    "            # Perform the one-sided paired t-test instead of the Wilcoxon test.\n",
    "            # H1: The mean of scores_imputed is greater than the mean of scores_base.\n",
    "            stat, p_value = ttest_rel(scores_imputed, scores_base, alternative='greater')\n",
    "            # --------------------------------\n",
    "\n",
    "            statistical_results.append({\n",
    "                'test_condition': condition,\n",
    "                'comparison': comp_name,\n",
    "                'mean_gain': mean_gain,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "\n",
    "    if not statistical_results:\n",
    "        print(\"No valid comparisons could be made.\")\n",
    "        return None\n",
    "\n",
    "    # --- Apply Multiple Testing Correction (this part is unchanged) ---\n",
    "    stats_df = pd.DataFrame(statistical_results)\n",
    "    \n",
    "    reject, p_adj, _, _ = multipletests(stats_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "    \n",
    "    stats_df['p_adj_fdr'] = p_adj\n",
    "    stats_df['significant_gain'] = reject\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Load the results file you saved from the previous run\n",
    "    try:\n",
    "        results_file = '../../results/downstream/task_05_imputing_test_set/results.csv'\n",
    "        all_results = pd.read_csv(results_file)\n",
    "        print(f\"Successfully loaded results from '{results_file}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The results file '{results_file}' was not found.\")\n",
    "        print(\"Please run the previous script first to generate the results.\")\n",
    "        exit()\n",
    "\n",
    "    # Analyze the results based on Balanced Accuracy\n",
    "    print(\"\\n\\n===== Analyzing Significance for Balanced Accuracy (using Paired T-Test) =====\")\n",
    "    significance_df_accuracy = analyze_significance_with_ttest(all_results, metric='balanced_accuracy')\n",
    "\n",
    "    if significance_df_accuracy is not None:\n",
    "        sorted_accuracy_results = significance_df_accuracy.sort_values(\n",
    "            by=['significant_gain', 'p_adj_fdr'], \n",
    "            ascending=[False, True]\n",
    "        )\n",
    "        print(\"The table below shows if an imputation method provided a statistically significant gain over ablation.\")\n",
    "        print(\"A 'significant_gain' of True means we can be confident the improvement was not due to random chance.\")\n",
    "        print(sorted_accuracy_results.to_string())\n",
    "    \n",
    "    # Analyze the results based on Macro F1-Score\n",
    "    print(\"\\n\\n===== Analyzing Significance for Macro F1-Score (using Paired T-Test) =====\")\n",
    "    significance_df_f1 = analyze_significance_with_ttest(all_results, metric='macro_f1_score')\n",
    "\n",
    "    if significance_df_f1 is not None:\n",
    "        sorted_f1_results = significance_df_f1.sort_values(\n",
    "            by=['significant_gain', 'p_adj_fdr'], \n",
    "            ascending=[False, True]\n",
    "        )\n",
    "        print(\"The table below shows if an imputation method provided a statistically significant gain over ablation.\")\n",
    "        print(\"A 'significant_gain' of True means we can be confident the improvement was not due to random chance.\")\n",
    "        print(sorted_f1_results.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f1961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e1b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "# NEW: Import standard imputers\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# --- SECTION 1: HELPER FUNCTIONS (NOW WITH STANDARD IMPUTERS) ---\n",
    "\n",
    "def load_and_prepare_pancancer_data(labels_dir: str, data_dir: str):\n",
    "    \"\"\"Loads and aligns all data for a pan-cancer analysis.\"\"\"\n",
    "    print(\"--- Loading and preparing all pan-cancer data... ---\")\n",
    "    X_train_orig = pd.read_csv(os.path.join(data_dir, \"real_data_train.csv\"), index_col=0)\n",
    "    X_test_orig = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"), index_col=0)\n",
    "    \n",
    "    # Corrected column name to 'pathologic_stage'\n",
    "    train_stage_df = pd.read_csv(os.path.join(labels_dir, \"train_stage.csv\"), index_col=0)\n",
    "    train_type_df = pd.read_csv(os.path.join(labels_dir, \"train_cancer_type.csv\"), index_col=0)\n",
    "    test_stage_df = pd.read_csv(os.path.join(labels_dir, \"test_stage.csv\"), index_col=0)\n",
    "    test_type_df = pd.read_csv(os.path.join(labels_dir, \"test_cancer_type.csv\"), index_col=0)\n",
    "    \n",
    "    train_labels_combined = train_stage_df.join(train_type_df).dropna(subset=['stage', 'cancertype'])\n",
    "    test_labels_combined = test_stage_df.join(test_type_df).dropna(subset=['stage', 'cancertype'])\n",
    "\n",
    "    train_common_idx = train_labels_combined.index.intersection(X_train_orig.index)\n",
    "    test_common_idx = test_labels_combined.index.intersection(X_test_orig.index)\n",
    "\n",
    "    X_train = X_train_orig.loc[train_common_idx].sort_index()\n",
    "    y_train = train_labels_combined.loc[train_common_idx, 'stage'].sort_index()\n",
    "    train_types = train_labels_combined.loc[train_common_idx, 'cancertype'].sort_index()\n",
    "\n",
    "    X_test = X_test_orig.loc[test_common_idx].sort_index()\n",
    "    y_test = test_labels_combined.loc[test_common_idx, 'stage'].sort_index()\n",
    "    test_types = test_labels_combined.loc[test_common_idx, 'cancertype'].sort_index()\n",
    "\n",
    "    print(f\"  Found {len(X_train)} training samples and {len(X_test)} test samples across all cancers.\")\n",
    "    \n",
    "    train_cancer_dummies = pd.get_dummies(train_types, prefix='cancer')\n",
    "    test_cancer_dummies = pd.get_dummies(test_types, prefix='cancer')\n",
    "    train_cancer_dummies, test_cancer_dummies = train_cancer_dummies.align(test_cancer_dummies, join='outer', axis=1, fill_value=0)\n",
    "    \n",
    "    X_train_final = pd.concat([X_train, train_cancer_dummies], axis=1)\n",
    "    X_test_final = pd.concat([X_test, test_cancer_dummies], axis=1)\n",
    "    \n",
    "    return X_train_final, y_train, X_test_final, y_test\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    \"\"\"Helper function to calculate and return all desired metrics.\"\"\"\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    macro_f1 = report['macro avg']['f1-score']\n",
    "    return balanced_acc, macro_f1\n",
    "\n",
    "\n",
    "# --- SECTION 2: MAIN ANALYSIS PIPELINE (MODIFIED TO USE STANDARD IMPUTERS) ---\n",
    "\n",
    "def run_pancancer_analysis_with_standard_imputation(random_seed: int):\n",
    "    \"\"\"\n",
    "    Trains a classifier and tests it against ablated and standard imputed data.\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test = load_and_prepare_pancancer_data(\n",
    "        \"../../datasets_TCGA/downstream_labels\",\n",
    "        \"./data_task_02\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Training pan-cancer model with random_state={random_seed}... ---\")\n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=random_seed, n_jobs=-1)\n",
    "    # The classifier is trained on the original data with NaNs, as this is the 'ablation' baseline\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    all_prefixes = {col.split('_')[0] for col in X_test.columns if '_' in col}\n",
    "    possible_modalities = ['cna', 'rnaseq', 'rppa', 'wsi'] \n",
    "    available_modalities = sorted([m for m in possible_modalities if m in all_prefixes])\n",
    "    print(f\"  Available modalities for ablation/imputation: {available_modalities}\")\n",
    "\n",
    "    all_results = []\n",
    "    \n",
    "    # --- Step 1 - Evaluate on the full, unmodified test set first ---\n",
    "    print(\"\\n--- Processing Test Condition: full_data ---\")\n",
    "    y_pred_full = classifier.predict(X_test)\n",
    "    b_acc, f1 = get_metrics(y_test, y_pred_full)\n",
    "    all_results.append({\n",
    "        'test_condition': 'full_data',\n",
    "        'test_type': 'full_data',\n",
    "        'balanced_accuracy': b_acc,\n",
    "        'macro_f1_score': f1\n",
    "    })\n",
    "\n",
    "    # --- Step 2 - Loop through combinations of modalities to remove/impute ---\n",
    "    for r in range(1, len(available_modalities) + 1):\n",
    "        for combo in combinations(available_modalities, r):\n",
    "            \n",
    "            if len(combo) == len(available_modalities):\n",
    "                condition_name = \"cancer_label_only\"\n",
    "            else:\n",
    "                condition_name = f\"no_{'_'.join(combo)}\"\n",
    "            \n",
    "            print(f\"\\n--- Processing Test Condition: {condition_name} ---\")\n",
    "\n",
    "            cols_to_nullify = [col for mod in combo for col in X_test.columns if col.startswith(mod + '_')]\n",
    "            X_test_ablated = X_test.copy()\n",
    "            X_test_ablated[cols_to_nullify] = np.nan\n",
    "            \n",
    "            # --- Define the imputation strategies ---\n",
    "            imputation_strategies = {\n",
    "                'ablation': X_test_ablated,\n",
    "            }\n",
    "\n",
    "            # --- Mean Imputation ---\n",
    "            print(\"    Imputing with 'mean'...\")\n",
    "            mean_imputer = SimpleImputer(strategy='mean')\n",
    "            # Fit *only* on the training data to learn the means\n",
    "            mean_imputer.fit(X_train)\n",
    "            # Transform the ablated test data\n",
    "            X_test_imputed_mean_np = mean_imputer.transform(X_test_ablated)\n",
    "            imputation_strategies['imputed_mean'] = pd.DataFrame(X_test_imputed_mean_np, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "            # --- KNN Imputation ---\n",
    "            print(\"    Imputing with 'knn'...\")\n",
    "            knn_imputer = KNNImputer(n_neighbors=5)\n",
    "            # Fit *only* on the training data to learn the KNN model\n",
    "            knn_imputer.fit(X_train)\n",
    "            # Transform the ablated test data\n",
    "            X_test_imputed_knn_np = knn_imputer.transform(X_test_ablated)\n",
    "            imputation_strategies['imputed_knn'] = pd.DataFrame(X_test_imputed_knn_np, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "            # --- Evaluate each strategy ---\n",
    "            for test_type, X_test_current in imputation_strategies.items():\n",
    "                y_pred = classifier.predict(X_test_current)\n",
    "                b_acc, f1 = get_metrics(y_test, y_pred)\n",
    "                all_results.append({\n",
    "                    'test_condition': condition_name,\n",
    "                    'test_type': test_type,\n",
    "                    'balanced_accuracy': b_acc,\n",
    "                    'macro_f1_score': f1\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# --- SECTION 3: MAIN EXECUTION AND VISUALIZATION ---\n",
    "\n",
    "def create_summary_plot(data: pd.DataFrame, metric: str, title: str):\n",
    "    \"\"\"Helper function to create a consistent plot for a given metric.\"\"\"\n",
    "    # ... (code is unchanged)\n",
    "    print(f\"\\n--- Generating plot for: {metric} ---\")\n",
    "    data['n_removed'] = data['test_condition'].apply(lambda x: 0 if x == 'full_data' else x.count('_') + 1)\n",
    "    data.loc[data['test_condition'] == 'cancer_label_only', 'n_removed'] = data['n_removed'].max() + 1\n",
    "    plot_order = data.sort_values(by=['n_removed', 'test_condition']).test_condition.unique()\n",
    "    g = sns.catplot(\n",
    "        data=data, x='test_condition', y=metric, hue='test_type',\n",
    "        order=plot_order, kind='bar', height=6, aspect=2.5, legend_out=False, errorbar='sd'\n",
    "    )\n",
    "    g.fig.suptitle(title, y=1.03, fontsize=16)\n",
    "    g.set_axis_labels(\"Test Condition (Modalities Removed / Available)\", f\"Mean {metric.replace('_', ' ').title()}\")\n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    plt.legend(title='Test Type')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    N_RUNS = 5\n",
    "    all_run_dfs = []\n",
    "\n",
    "    for i in range(N_RUNS):\n",
    "        print(f\"\\n{'='*25} Starting Run {i+1}/{N_RUNS} {'='*25}\")\n",
    "        results_df = run_pancancer_analysis_with_standard_imputation(random_seed=i)\n",
    "        \n",
    "        if results_df is not None:\n",
    "            results_df['run'] = i + 1\n",
    "            all_run_dfs.append(results_df)\n",
    "\n",
    "    if all_run_dfs:\n",
    "        final_results = pd.concat(all_run_dfs, ignore_index=True)\n",
    "        \n",
    "\n",
    "        print(\"\\n\\n===== SUMMARY STATISTICS ACROSS ALL RUNS =====\")\n",
    "        summary_stats = final_results.groupby(['test_condition', 'test_type'])[['balanced_accuracy', 'macro_f1_score']].agg(\n",
    "            ['mean', 'std', 'median']\n",
    "        )\n",
    "        summary_stats['n_removed'] = summary_stats.index.get_level_values('test_condition').map(\n",
    "            lambda x: 0 if x == 'full_data' else (x.count('_') + 2 if x == 'cancer_label_only' else x.count('_') + 1)\n",
    "        )\n",
    "        print(summary_stats.sort_values(by=['n_removed', ('balanced_accuracy', 'mean')], ascending=[True, False]).drop(columns='n_removed').to_string())\n",
    "\n",
    "        create_summary_plot(final_results, 'balanced_accuracy', 'Comparison of Standard Imputation Strategies (Balanced Accuracy)')\n",
    "        create_summary_plot(final_results, 'macro_f1_score', 'Comparison of Standard Imputation Strategies (Macro F1-Score)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a029119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_results.to_csv('../../results/downstream/task_05_imputing_test_set/standard_imputation_results.csv', index=False)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaaa34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# The analysis functions from the previous step are unchanged.\n",
    "def analyze_vs_ablation(combined_df: pd.DataFrame, metric: str = 'balanced_accuracy'):\n",
    "    \"\"\"\n",
    "    Performs statistical analysis comparing all imputation methods against the ablation baseline.\n",
    "    \"\"\"\n",
    "    statistical_results = []\n",
    "    test_conditions = [c for c in combined_df['test_condition'].unique() if c not in ['full_data', 'cancer_label_only']]\n",
    "\n",
    "    for condition in test_conditions:\n",
    "        condition_df = combined_df[combined_df['test_condition'] == condition]\n",
    "        try:\n",
    "            scores_ablation = condition_df[condition_df['test_type'] == 'ablation'][metric].dropna()\n",
    "            imputation_types = [t for t in condition_df['test_type'].unique() if 'imputed' in t]\n",
    "            for imp_type in imputation_types:\n",
    "                scores_imputed = condition_df[condition_df['test_type'] == imp_type][metric].dropna()\n",
    "                if len(scores_imputed) != len(scores_ablation) or len(scores_imputed) < 2: continue\n",
    "                mean_gain = scores_imputed.mean() - scores_ablation.mean()\n",
    "                stat, p_value = ttest_rel(scores_imputed, scores_ablation, alternative='greater')\n",
    "                statistical_results.append({\n",
    "                    'test_condition': condition,\n",
    "                    'comparison': f\"{imp_type} vs. ablation\",\n",
    "                    'mean_gain': mean_gain,\n",
    "                    'p_value': p_value\n",
    "                })\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "\n",
    "    if not statistical_results: return None\n",
    "    stats_df = pd.DataFrame(statistical_results)\n",
    "    reject, p_adj, _, _ = multipletests(stats_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "    stats_df['p_adj_fdr'] = p_adj\n",
    "    stats_df['significant_gain'] = reject\n",
    "    return stats_df\n",
    "\n",
    "def analyze_generative_vs_standard(combined_df: pd.DataFrame, metric: str = 'balanced_accuracy'):\n",
    "    \"\"\"\n",
    "    Performs statistical analysis comparing generative models against standard imputation methods.\n",
    "    \"\"\"\n",
    "    statistical_results = []\n",
    "    test_conditions = [c for c in combined_df['test_condition'].unique() if c not in ['full_data', 'cancer_label_only']]\n",
    "\n",
    "    for condition in test_conditions:\n",
    "        condition_df = combined_df[combined_df['test_condition'] == condition]\n",
    "        try:\n",
    "            scores_mean = condition_df[condition_df['test_type'] == 'imputed_mean'][metric].dropna()\n",
    "            scores_knn = condition_df[condition_df['test_type'] == 'imputed_knn'][metric].dropna()\n",
    "            scores_multi = condition_df[condition_df['test_type'] == 'imputed_multi'][metric].dropna()\n",
    "            scores_coherent = condition_df[condition_df['test_type'] == 'imputed_coherent'][metric].dropna()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        standard_models = {'mean': scores_mean, 'knn': scores_knn}\n",
    "        generative_models = {'multi': scores_multi, 'coherent': scores_coherent}\n",
    "\n",
    "        for gen_name, gen_scores in generative_models.items():\n",
    "            for std_name, std_scores in standard_models.items():\n",
    "                if len(gen_scores) != len(std_scores) or len(gen_scores) < 2: continue\n",
    "                \n",
    "                mean_gain = gen_scores.mean() - std_scores.mean()\n",
    "                stat, p_value = ttest_rel(gen_scores, std_scores, alternative='greater')\n",
    "                \n",
    "                statistical_results.append({\n",
    "                    'test_condition': condition,\n",
    "                    'comparison': f\"imputed_{gen_name} vs. imputed_{std_name}\",\n",
    "                    'mean_gain': mean_gain,\n",
    "                    'p_value': p_value\n",
    "                })\n",
    "                \n",
    "    if not statistical_results: return None\n",
    "    stats_df = pd.DataFrame(statistical_results)\n",
    "    reject, p_adj, _, _ = multipletests(stats_df['p_value'], alpha=0.05, method='fdr_bh')\n",
    "    stats_df['p_adj_fdr'] = p_adj\n",
    "    stats_df['significant_gain'] = reject\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "# --- CORRECTED PLOTTING FUNCTION ---\n",
    "def create_final_comparison_plot(data: pd.DataFrame, metric: str, title: str, save_path= None):\n",
    "    \"\"\"Creates the final comparison plot with a custom color palette and legend placement.\"\"\"\n",
    "    print(f\"\\n--- Generating final comparison plot for: {metric} ---\")\n",
    "    \n",
    "    condition_to_exclude = (data['test_condition'] == 'cancer_label_only') & (data['test_type'] != 'ablation')\n",
    "    plot_data = data[~condition_to_exclude].copy()\n",
    "\n",
    "    plot_data['n_removed'] = plot_data['test_condition'].apply(\n",
    "        lambda x: 0 if x == 'full_data' else (99 if x == 'cancer_label_only' else x.count('_') + 1)\n",
    "    )\n",
    "    plot_order = plot_data.sort_values(by=['n_removed', 'test_condition']).test_condition.unique()\n",
    "    \n",
    "    palette = {\n",
    "        'full_data': \"#67F122\",\n",
    "        'ablation': \"#E7C923\",\n",
    "        'imputed_mean': \"#EE8DCC\",\n",
    "        'imputed_knn': \"#AD2C9C\",\n",
    "        'imputed_multi': \"#115887\",\n",
    "        'imputed_coherent': \"#14C1DC\"\n",
    "    }\n",
    "    \n",
    "    hue_order = ['full_data', 'ablation', 'imputed_mean', 'imputed_knn', 'imputed_multi', 'imputed_coherent']\n",
    "    plot_data_hue_order = [h for h in hue_order if h in plot_data['test_type'].unique()]\n",
    "    \n",
    "    g = sns.catplot(\n",
    "        data=plot_data, x='test_condition', y=metric, hue='test_type',\n",
    "        order=plot_order, hue_order=plot_data_hue_order, kind='bar', height=7, aspect=2.2,\n",
    "        palette=palette,\n",
    "        errorbar='sd',\n",
    "        # THIS IS THE FIX: Remove legend=False. Let catplot create the legend.\n",
    "        # legend=False \n",
    "    )\n",
    "    \n",
    "    # Now that the legend exists, we can move it.\n",
    "    sns.move_legend(\n",
    "        g, \"center right\",\n",
    "        bbox_to_anchor=(1.1, 0.5), \n",
    "        frameon=True,\n",
    "        title='Test Type'\n",
    "    )\n",
    "\n",
    "    g.fig.suptitle(title, y=1.03, fontsize=18)\n",
    "    g.set_axis_labels(\"Test Condition (Modalities Removed)\", f\"Mean {metric.replace('_', ' ').title()}\", fontsize=14)\n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    if save_path is not None:\n",
    "        g.savefig(save_path)\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # 1. Load both result files\n",
    "    try:\n",
    "        results_generative_file = '../../results/downstream/task_05_imputing_test_set/results.csv'\n",
    "        df_generative = pd.read_csv(results_generative_file)\n",
    "        print(f\"Successfully loaded generative results from '{results_generative_file}'\")\n",
    "        \n",
    "        results_standard_file = '../../results/downstream/task_05_imputing_test_set/standard_imputation_results.csv'\n",
    "        df_standard = pd.read_csv(results_standard_file)\n",
    "        print(f\"Successfully loaded standard imputation results from '{results_standard_file}'\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find a results file. {e}\")\n",
    "        print(\"Please ensure both 'pancancer_imputation_comparison_results.csv' and 'pancancer_standard_imputation_results.csv' are present.\")\n",
    "        exit()\n",
    "\n",
    "    df_standard_subset = df_standard[df_standard['test_type'].isin(['ablation', 'imputed_mean', 'imputed_knn', 'full_data'])]\n",
    "    df_generative_subset = df_generative[df_generative['test_type'].isin(['imputed_multi', 'imputed_coherent'])]\n",
    "    final_combined_results = pd.concat([df_standard_subset, df_generative_subset], ignore_index=True)\n",
    "    print(\"\\nDataFrames successfully combined.\")\n",
    "    \n",
    "    # --- MODIFIED: Print the full sorted tables, not just the significant results ---\n",
    "    print(\"\\n\\n===== Analysis 1: Significance of Gain vs. Ablation (Balanced Accuracy) =====\")\n",
    "    significance_vs_ablation = analyze_vs_ablation(final_combined_results, metric='balanced_accuracy')\n",
    "    if significance_vs_ablation is not None:\n",
    "        sorted_results_1 = significance_vs_ablation.sort_values(by=['significant_gain', 'p_adj_fdr'], ascending=[False, True])\n",
    "        print(sorted_results_1.to_string())\n",
    "    \n",
    "    print(\"\\n\\n===== Analysis 2: Significance of Gain for Generative vs. Standard Methods (Balanced Accuracy) =====\")\n",
    "    significance_gen_vs_std = analyze_generative_vs_standard(final_combined_results, metric='balanced_accuracy')\n",
    "    if significance_gen_vs_std is not None:\n",
    "        sorted_results_2 = significance_gen_vs_std.sort_values(by=['significant_gain', 'p_adj_fdr'], ascending=[False, True])\n",
    "        print(sorted_results_2.to_string())\n",
    "\n",
    "    results_path = '../../results/downstream/task_05_imputing_test_set'\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    \n",
    "    # --- Final Visualization ---\n",
    "    create_final_comparison_plot(final_combined_results, 'balanced_accuracy', 'Final Comparison of All Imputation Strategies (Balanced Accuracy)', save_path=os.path.join(results_path, 'balanced_accuracy_imputations.png'))\n",
    "    create_final_comparison_plot(final_combined_results, 'macro_f1_score', 'Final Comparison of All Imputation Strategies (Macro F1-Score)', save_path=os.path.join(results_path, 'f1_score_imputations.png'))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
