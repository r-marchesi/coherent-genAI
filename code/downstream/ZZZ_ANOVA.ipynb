{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ffe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "def parse_results_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Parses the JSON results file and converts it into a long-format pandas DataFrame\n",
    "    suitable for repeated measures ANOVA.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON results file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with columns for repetition, modality, task,\n",
    "                          data_type, balanced_accuracy, and f1_macro.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            all_runs_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        print(\"Please make sure the file path is correct.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file at {file_path} is not a valid JSON file.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # A list to hold all the records before creating the DataFrame\n",
    "    records = []\n",
    "\n",
    "    # Enumerate through the outer list to get the repetition number\n",
    "    for rep_idx, run_data in enumerate(all_runs_data):\n",
    "        # Each 'run_data' is a list of dictionaries, one for each modality\n",
    "        for modality_results in run_data:\n",
    "            modality_name = modality_results['modality']\n",
    "            \n",
    "            # Iterate over the two tasks: 'tumor_type' and 'stage'\n",
    "            for task_name in ['tumor_type', 'stage']:\n",
    "                if task_name in modality_results:\n",
    "                    task_data = modality_results[task_name]\n",
    "                    \n",
    "                    # Iterate over the data types: 'real', 'synthetic_from_coherent', etc.\n",
    "                    for data_type_name, metrics in task_data.items():\n",
    "                        record = {\n",
    "                            'repetition': rep_idx,\n",
    "                            'modality': modality_name,\n",
    "                            'task': task_name,\n",
    "                            'data_type': data_type_name,\n",
    "                            'balanced_accuracy': metrics.get('balanced_accuracy'),\n",
    "                            'f1_macro': metrics.get('f1_macro')\n",
    "                        }\n",
    "                        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "def perform_anova(df, task, metric):\n",
    "    \"\"\"\n",
    "    Performs a two-way repeated measures ANOVA and prints the results.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the data.\n",
    "        task (str): The task to filter by ('tumor_type' or 'stage').\n",
    "        metric (str): The dependent variable metric ('balanced_accuracy' or 'f1_macro').\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Performing Repeated Measures ANOVA for:\")\n",
    "    print(f\"  Task   : {task}\")\n",
    "    print(f\"  Metric : {metric}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Filter the DataFrame for the specific task\n",
    "    task_df = df[df['task'] == task].copy()\n",
    "\n",
    "    # Check if the filtered dataframe is empty\n",
    "    if task_df.empty:\n",
    "        print(f\"\\nWarning: No data found for task '{task}'. Skipping ANOVA.\\n\")\n",
    "        return\n",
    "\n",
    "    # Perform the repeated measures ANOVA\n",
    "    # The 'subject' is the identifier for each independent repetition.\n",
    "    # 'within' specifies the within-subject factors ('modality' and 'data_type').\n",
    "    try:\n",
    "        aov = AnovaRM(data=task_df,\n",
    "                      depvar=metric,\n",
    "                      subject='repetition',\n",
    "                      within=['modality', 'data_type'])\n",
    "        \n",
    "        res = aov.fit()\n",
    "        print(res.summary())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during ANOVA calculation for {task} - {metric}: {e}\\n\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path to your JSON file\n",
    "    # IMPORTANT: Update this path to point to your actual file location.\n",
    "    file_path = '../../results/downstream/task_01_train_on_real/evaluation_results_10_runs.json'\n",
    "\n",
    "    # 1. Parse the data\n",
    "    results_df = parse_results_to_dataframe(file_path)\n",
    "\n",
    "    if results_df is not None:\n",
    "        # 2. Define the tasks and metrics for which to run the anova\n",
    "        tasks_to_analyze = ['tumor_type', 'stage']\n",
    "        metrics_to_analyze = ['balanced_accuracy', 'f1_macro']\n",
    "\n",
    "        # 3. Run the ANOVAs\n",
    "        for task in tasks_to_analyze:\n",
    "            for metric in metrics_to_analyze:\n",
    "                perform_anova(results_df, task, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73119f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "def parse_results_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Parses the JSON results file and converts it into a long-format pandas DataFrame.\n",
    "    (This is the same function from the previous step).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            all_runs_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        return None\n",
    "    records = []\n",
    "    for rep_idx, run_data in enumerate(all_runs_data):\n",
    "        for modality_results in run_data:\n",
    "            modality_name = modality_results['modality']\n",
    "            for task_name in ['tumor_type', 'stage']:\n",
    "                if task_name in modality_results:\n",
    "                    task_data = modality_results[task_name]\n",
    "                    for data_type_name, metrics in task_data.items():\n",
    "                        record = {\n",
    "                            'repetition': rep_idx, 'modality': modality_name,\n",
    "                            'task': task_name, 'data_type': data_type_name,\n",
    "                            'balanced_accuracy': metrics.get('balanced_accuracy'),\n",
    "                            'f1_macro': metrics.get('f1_macro')\n",
    "                        }\n",
    "                        records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '../../results/downstream/task_01_train_on_real/evaluation_results_10_runs.json'\n",
    "    results_df = parse_results_to_dataframe(file_path)\n",
    "\n",
    "    if results_df is not None:\n",
    "        # Define parameters for the analysis\n",
    "        tasks_to_analyze = ['tumor_type', 'stage']\n",
    "        metrics_to_analyze = ['balanced_accuracy', 'f1_macro']\n",
    "        modalities = results_df['modality'].unique()\n",
    "        data_types = ['real', 'synthetic_from_coherent', 'synthetic_from_multi']\n",
    "\n",
    "        # Define the number of comparisons for Bonferroni correction\n",
    "        n_comparisons = 3 # (real vs coherent), (real vs multi), (coherent vs multi)\n",
    "        alpha = 0.05\n",
    "        corrected_alpha = alpha / n_comparisons\n",
    "        print(f\"Using Bonferroni-corrected alpha = {corrected_alpha:.4f}\\n\")\n",
    "\n",
    "        # Loop through each task and metric\n",
    "        for task in tasks_to_analyze:\n",
    "            for metric in metrics_to_analyze:\n",
    "                print(\"=\"*80)\n",
    "                print(f\"ANALYSIS: Task = {task.upper()}, Metric = {metric.upper()}\")\n",
    "                print(\"=\"*80)\n",
    "                \n",
    "                # Perform comparisons within each modality\n",
    "                for modality in modalities:\n",
    "                    print(f\"\\n--- Modality: {modality} ---\")\n",
    "                    \n",
    "                    modality_df = results_df[\n",
    "                        (results_df['task'] == task) &\n",
    "                        (results_df['modality'] == modality)\n",
    "                    ]\n",
    "\n",
    "                    # Create a dictionary to hold the series of results for each data type\n",
    "                    results_by_dtype = {\n",
    "                        dtype: modality_df[modality_df['data_type'] == dtype][metric]\n",
    "                        for dtype in data_types\n",
    "                    }\n",
    "\n",
    "                    # Perform pairwise paired t-tests\n",
    "                    for dt1, dt2 in combinations(data_types, 2):\n",
    "                        # Get the two series to compare\n",
    "                        series1 = results_by_dtype[dt1]\n",
    "                        series2 = results_by_dtype[dt2]\n",
    "\n",
    "                        # Perform the paired t-test\n",
    "                        t_stat, p_value = stats.ttest_rel(series1, series2)\n",
    "\n",
    "                        # Check for significance against the corrected alpha\n",
    "                        is_significant = \"Significant\" if p_value < corrected_alpha else \"Not Significant\"\n",
    "\n",
    "                        print(f\"{dt1:>25} vs. {dt2:<25}: p-value = {p_value:.6f} ({is_significant})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03356f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5ef6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde59d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c102f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "def run_anova_for_imputation(df, metric):\n",
    "    \"\"\"\n",
    "    Performs and prints a two-way repeated measures ANOVA.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ANOVA Results for Metric: {metric.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        aov = AnovaRM(data=df,\n",
    "                      depvar=metric,\n",
    "                      subject='run', # 'run' identifies the repetition\n",
    "                      within=['test_condition', 'test_type'])\n",
    "        \n",
    "        res = aov.fit()\n",
    "        print(res.summary())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during ANOVA calculation for {metric}: {e}\\n\")\n",
    "\n",
    "def run_posthoc_ttests_for_imputation(df, metric):\n",
    "    \"\"\"\n",
    "    Performs and prints post-hoc paired t-tests with Bonferroni correction.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Post-Hoc Paired t-test Results for Metric: {metric.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    conditions = df['test_condition'].unique()\n",
    "    test_types = ['ablation', 'imputed_coherent', 'imputed_multi']\n",
    "\n",
    "    # Define the number of comparisons for Bonferroni correction\n",
    "    n_conditions = len(conditions)\n",
    "    n_pairs_per_condition = 3\n",
    "    n_comparisons = n_conditions * n_pairs_per_condition\n",
    "\n",
    "    alpha = 0.05\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"Using Bonferroni-corrected alpha = {corrected_alpha:.4f} for significance.\\n\")\n",
    "\n",
    "    # Perform comparisons within each test_condition\n",
    "    for condition in conditions:\n",
    "        print(f\"\\n--- Condition: {condition} ---\")\n",
    "        \n",
    "        condition_df = df[df['test_condition'] == condition]\n",
    "\n",
    "        # Create a dictionary to hold the series of results for each test type\n",
    "        results_by_ttype = {\n",
    "            ttype: condition_df[condition_df['test_type'] == ttype][metric]\n",
    "            for ttype in test_types\n",
    "        }\n",
    "\n",
    "        # Perform pairwise paired t-tests\n",
    "        for tt1, tt2 in combinations(test_types, 2):\n",
    "            # Get the two series to compare\n",
    "            series1 = results_by_ttype[tt1]\n",
    "            series2 = results_by_ttype[tt2]\n",
    "\n",
    "            # Check if either series is empty (can happen if a combination doesn't exist)\n",
    "            if series1.empty or series2.empty:\n",
    "                continue\n",
    "\n",
    "            # Perform the paired t-test\n",
    "            t_stat, p_value = stats.ttest_rel(series1, series2)\n",
    "\n",
    "            # Check for significance against the corrected alpha\n",
    "            is_significant = \"Significant\" if p_value < corrected_alpha else \"Not Significant\"\n",
    "\n",
    "            print(f\"{tt1:>18} vs. {tt2:<18}: p-value = {p_value:.6f} ({is_significant})\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load the dataset from the CSV file\n",
    "    try:\n",
    "        file_path = '../../results/downstream/task_05_imputing_test_set/results_10_runs.csv'\n",
    "        full_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Filter the data as requested\n",
    "    conditions_to_exclude = ['cancer_label_only', 'full_data']\n",
    "    filtered_df = full_df[~full_df['test_condition'].isin(conditions_to_exclude)].copy()\n",
    "    \n",
    "    print(f\"Successfully loaded and filtered data. Kept {len(filtered_df)} rows for analysis.\\n\")\n",
    "\n",
    "    # Define the metrics to analyze\n",
    "    metrics_to_analyze = ['balanced_accuracy', 'macro_f1_score']\n",
    "\n",
    "    for metric in metrics_to_analyze:\n",
    "        # 3. Run the Repeated Measures ANOVA\n",
    "        run_anova_for_imputation(filtered_df, metric)\n",
    "        \n",
    "        # 4. Run the Post-Hoc t-tests to explore 'test_type' differences\n",
    "        run_posthoc_ttests_for_imputation(filtered_df, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def run_imputation_vs_full_data_ttests(df, metric):\n",
    "    \"\"\"\n",
    "    Compares imputed data against the full_data baseline using paired t-tests.\n",
    "    If a result is significant, it also reports which group performed better.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ANALYSIS: Imputation vs. Full Data for Metric: {metric.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Isolate the baseline 'full_data' results\n",
    "    full_data_results = df[df['test_condition'] == 'full_data'].sort_values('run')[metric]\n",
    "    if full_data_results.empty:\n",
    "        print(\"Error: Could not find 'full_data' in the dataset. Aborting.\")\n",
    "        return\n",
    "    mean_full_data = full_data_results.mean()\n",
    "\n",
    "    # Step 2: Get the data for imputed conditions\n",
    "    imputed_df = df[df['test_type'].isin(['imputed_coherent', 'imputed_multi'])]\n",
    "    conditions_to_test = imputed_df['test_condition'].unique()\n",
    "\n",
    "    # Step 3: Set up the Bonferroni correction\n",
    "    n_conditions = len(conditions_to_test)\n",
    "    n_pairs_per_condition = 2\n",
    "    n_comparisons = n_conditions * n_pairs_per_condition    \n",
    "    alpha = 0.05\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"Using Bonferroni-corrected alpha = {corrected_alpha:.4f} for significance.\\n\")\n",
    "\n",
    "    # Step 4: Loop through each condition and perform the comparisons\n",
    "    for condition in conditions_to_test:\n",
    "        print(f\"\\n--- Comparing Condition '{condition}' against Full Data ---\")\n",
    "        condition_df = imputed_df[imputed_df['test_condition'] == condition]\n",
    "\n",
    "        imputation_types_to_test = ['imputed_coherent', 'imputed_multi']\n",
    "        for imp_type in imputation_types_to_test:\n",
    "            imputed_results = condition_df[condition_df['test_type'] == imp_type].sort_values('run')[metric]\n",
    "\n",
    "            if not imputed_results.empty:\n",
    "                # Perform the paired t-test\n",
    "                t_stat, p_value = stats.ttest_rel(full_data_results, imputed_results)\n",
    "                \n",
    "                significance_details = \"\"\n",
    "                # Check if the result is statistically significant\n",
    "                if p_value < corrected_alpha:\n",
    "                    mean_imputed = imputed_results.mean()\n",
    "                    # Determine which group had the higher mean score\n",
    "                    if mean_imputed > mean_full_data:\n",
    "                        winner_string = f\"{imp_type} is better\"\n",
    "                    else:\n",
    "                        winner_string = \"full_data is better\"\n",
    "                    significance_details = f\"Significant ({winner_string})\"\n",
    "                else:\n",
    "                    significance_details = \"Not Significant\"\n",
    "\n",
    "                print(f\"{imp_type:>18} vs. {'full_data':<18}: p-value = {p_value:.6f} ({significance_details})\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset from the CSV file\n",
    "    try:\n",
    "        file_path = '../../results/downstream/task_05_imputing_test_set/results_10_runs.csv'\n",
    "        full_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        exit()\n",
    "    \n",
    "    # Define the metrics to analyze\n",
    "    metrics_to_analyze = ['balanced_accuracy', 'macro_f1_score']\n",
    "\n",
    "    for metric in metrics_to_analyze:\n",
    "        run_imputation_vs_full_data_ttests(full_df, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "def run_anova_for_imputation(df, metric):\n",
    "    \"\"\"\n",
    "    Performs and prints a two-way repeated measures ANOVA.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ANOVA Results for Metric: {metric.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        aov = AnovaRM(data=df,\n",
    "                      depvar=metric,\n",
    "                      subject='run', # 'run' identifies the repetition\n",
    "                      within=['test_condition', 'test_type'])\n",
    "        \n",
    "        res = aov.fit()\n",
    "        print(res.summary())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during ANOVA calculation for {metric}: {e}\\n\")\n",
    "\n",
    "def run_posthoc_ttests_for_imputation(df, metric):\n",
    "    \"\"\"\n",
    "    Performs and prints post-hoc paired t-tests with Bonferroni correction.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Post-Hoc Paired t-test Results for Metric: {metric.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    conditions = df['test_condition'].unique()\n",
    "    test_types = ['ablation', 'imputed_coherent', 'imputed_multi']\n",
    "\n",
    "    # Define the number of comparisons for Bonferroni correction\n",
    "    n_conditions = len(conditions)\n",
    "    n_pairs_per_condition = 3\n",
    "    n_comparisons = n_conditions * n_pairs_per_condition\n",
    "    alpha = 0.05\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"Using Bonferroni-corrected alpha = {corrected_alpha:.4f} for significance.\\n\")\n",
    "\n",
    "    # Perform comparisons within each test_condition\n",
    "    for condition in conditions:\n",
    "        print(f\"\\n--- Condition: {condition} ---\")\n",
    "        \n",
    "        condition_df = df[df['test_condition'] == condition]\n",
    "\n",
    "        # Create a dictionary to hold the series of results for each test type\n",
    "        results_by_ttype = {\n",
    "            ttype: condition_df[condition_df['test_type'] == ttype][metric]\n",
    "            for ttype in test_types\n",
    "        }\n",
    "\n",
    "        # Perform pairwise paired t-tests\n",
    "        for tt1, tt2 in combinations(test_types, 2):\n",
    "            # Get the two series to compare\n",
    "            series1 = results_by_ttype[tt1]\n",
    "            series2 = results_by_ttype[tt2]\n",
    "\n",
    "            # Check if either series is empty (can happen if a combination doesn't exist)\n",
    "            if series1.empty or series2.empty:\n",
    "                continue\n",
    "\n",
    "            # Perform the paired t-test\n",
    "            t_stat, p_value = stats.ttest_rel(series1, series2)\n",
    "\n",
    "            # Check for significance against the corrected alpha\n",
    "            is_significant = \"Significant\" if p_value < corrected_alpha else \"Not Significant\"\n",
    "\n",
    "            print(f\"{tt1:>18} vs. {tt2:<18}: p-value = {p_value:.6f} ({is_significant})\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load the dataset from the CSV file\n",
    "    try:\n",
    "        file_path = '../../results/downstream/task_06_imputing_test_set_surv/all_imputations_results_long_rf.csv'\n",
    "        full_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Filter the data as requested\n",
    "    conditions_to_exclude = ['cancer_label_only', 'full_data']\n",
    "    filtered_df = full_df[~full_df['test_condition'].isin(conditions_to_exclude)].copy()\n",
    "    \n",
    "    print(f\"Successfully loaded and filtered data. Kept {len(filtered_df)} rows for analysis.\\n\")\n",
    "\n",
    "    # Define the metrics to analyze\n",
    "    metrics_to_analyze = ['c_index']\n",
    "\n",
    "    for metric in metrics_to_analyze:\n",
    "        # 3. Run the Repeated Measures ANOVA\n",
    "        run_anova_for_imputation(filtered_df, metric)\n",
    "        \n",
    "        # 4. Run the Post-Hoc t-tests to explore 'test_type' differences\n",
    "        run_posthoc_ttests_for_imputation(filtered_df, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def run_imputation_vs_full_data_ttests(df, metric):\n",
    "    \"\"\"\n",
    "    Compares imputed data against the full_data baseline using paired t-tests.\n",
    "    If a result is significant, it also reports which group performed better.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ANALYSIS: Imputation vs. Full Data for Metric: {metric.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Isolate the baseline 'full_data' results\n",
    "    full_data_results = df[df['test_condition'] == 'full_data'].sort_values('run')[metric]\n",
    "    if full_data_results.empty:\n",
    "        print(\"Error: Could not find 'full_data' in the dataset. Aborting.\")\n",
    "        return\n",
    "    mean_full_data = full_data_results.mean()\n",
    "\n",
    "    # Step 2: Get the data for imputed conditions\n",
    "    imputed_df = df[df['test_type'].isin(['imputed_coherent', 'imputed_multi'])]\n",
    "    conditions_to_test = imputed_df['test_condition'].unique()\n",
    "\n",
    "    # Step 3: Set up the Bonferroni correction\n",
    "    n_conditions = len(conditions_to_test)\n",
    "    n_pairs_per_condition = 2\n",
    "    n_comparisons = n_conditions * n_pairs_per_condition    \n",
    "    alpha = 0.05\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"Using Bonferroni-corrected alpha = {corrected_alpha:.4f} for significance.\\n\")\n",
    "\n",
    "    # Step 4: Loop through each condition and perform the comparisons\n",
    "    for condition in conditions_to_test:\n",
    "        print(f\"\\n--- Comparing Condition '{condition}' against Full Data ---\")\n",
    "        condition_df = imputed_df[imputed_df['test_condition'] == condition]\n",
    "\n",
    "        imputation_types_to_test = ['imputed_coherent', 'imputed_multi']\n",
    "        for imp_type in imputation_types_to_test:\n",
    "            imputed_results = condition_df[condition_df['test_type'] == imp_type].sort_values('run')[metric]\n",
    "\n",
    "            if not imputed_results.empty:\n",
    "                # Perform the paired t-test\n",
    "                t_stat, p_value = stats.ttest_rel(full_data_results, imputed_results)\n",
    "                \n",
    "                significance_details = \"\"\n",
    "                # Check if the result is statistically significant\n",
    "                if p_value < corrected_alpha:\n",
    "                    mean_imputed = imputed_results.mean()\n",
    "                    # Determine which group had the higher mean score\n",
    "                    if mean_imputed > mean_full_data:\n",
    "                        winner_string = f\"{imp_type} is better\"\n",
    "                    else:\n",
    "                        winner_string = \"full_data is better\"\n",
    "                    significance_details = f\"Significant ({winner_string})\"\n",
    "                else:\n",
    "                    significance_details = \"Not Significant\"\n",
    "\n",
    "                print(f\"{imp_type:>18} vs. {'full_data':<18}: p-value = {p_value:.6f} ({significance_details})\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset from the CSV file\n",
    "    try:\n",
    "        file_path = '../../results/downstream/task_06_imputing_test_set_surv/all_imputations_results_long_rf.csv'\n",
    "        full_df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        exit()\n",
    "    \n",
    "    # Define the metrics to analyze\n",
    "    metrics_to_analyze = ['c_index']\n",
    "\n",
    "    for metric in metrics_to_analyze:\n",
    "        run_imputation_vs_full_data_ttests(full_df, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d992b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a8611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e64bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def run_auc_ttest_analysis(raw_data_path, ablation_steps):\n",
    "    \"\"\"\n",
    "    Loads raw experiment data, calculates the Area Under the Curve (AUC) for each \n",
    "    run, and performs paired t-tests to compare the methods.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Statistical Analysis: Counterfactual Inference vs. Random Ablation\")\n",
    "    print(\"Metric: Area Under the F1-Score Curve (AUC)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        with open(raw_data_path, 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The results file was not found at '{raw_data_path}'\")\n",
    "        print(\"Please make sure you have run the experiment script first.\")\n",
    "        return\n",
    "\n",
    "    # Bonferroni correction for 3 comparisons:\n",
    "    # 1. random vs coherent\n",
    "    # 2. random vs multi\n",
    "    # 3. coherent vs multi\n",
    "    n_comparisons = 3\n",
    "    alpha = 0.05\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"Using Bonferroni-corrected alpha = {corrected_alpha:.4f} for significance.\\n\")\n",
    "\n",
    "    # Loop through each tested modality (e.g., 'rna', 'wsi')\n",
    "    for modality_key, raw_data in all_results.items():\n",
    "        print(f\"\\n--- Analysis for Ablated Modality: {modality_key.upper()} ---\")\n",
    "\n",
    "        # Step 1: Calculate AUC for each of the 10 runs for each method\n",
    "        auc_scores = {}\n",
    "        for method, runs_data in raw_data.items():\n",
    "            # For each run, calculate the area under the curve.\n",
    "            # Using sklearn.metrics.auc. np.trapz(y, x) would also work.\n",
    "            auc_scores[method] = [auc(ablation_steps, run_f1_scores) for run_f1_scores in runs_data]\n",
    "\n",
    "        # Step 2: Perform pairwise paired t-tests on the lists of AUC scores\n",
    "        methods_to_compare = ['random', 'coherent', 'multi']\n",
    "        for method1, method2 in combinations(methods_to_compare, 2):\n",
    "            scores1 = auc_scores[method1]\n",
    "            scores2 = auc_scores[method2]\n",
    "\n",
    "            # Perform the paired t-test\n",
    "            t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
    "            \n",
    "            significance_details = \"\"\n",
    "            # Check for significance against the corrected alpha\n",
    "            if p_value < corrected_alpha:\n",
    "                mean1 = np.mean(scores1)\n",
    "                mean2 = np.mean(scores2)\n",
    "                winner = method1 if mean1 > mean2 else method2\n",
    "                significance_details = f\"Significant ({winner} is better)\"\n",
    "            else:\n",
    "                significance_details = \"Not Significant\"\n",
    "\n",
    "            print(f\"{method1:>10} vs. {method2:<10}: p-value = {p_value:.6f} ({significance_details})\")\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Define the path to your NEW raw data file\n",
    "    RAW_DATA_PATH = \"../../results/downstream/task_07_counterfactual/rna_wsi_results_RAW.json\"\n",
    "    \n",
    "    # The ablation steps used in the experiment, needed for AUC calculation\n",
    "    ABLATION_STEPS = np.arange(0, 1.05, 0.05)\n",
    "\n",
    "    run_auc_ttest_analysis(RAW_DATA_PATH, ABLATION_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf85609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f9488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# The path to the JSON file created by the recalculation script.\n",
    "RAW_SCORES_FILE_PATH = \"../../results/downstream/task_00_rsquared/recalculated_r2_scores_RAW.json\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN VERIFICATION SCRIPT\n",
    "# =============================================================================\n",
    "def verify_and_print_summary(file_path: str):\n",
    "    \"\"\"\n",
    "    Loads the raw scores JSON file and prints a summary table of means and STDs.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Verification: Calculating Summary Statistics from Raw Scores\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            all_raw_scores = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Verification failed: The file '{file_path}' was not found.\")\n",
    "        print(\"Please run the 'r2_recalculation_script' first.\")\n",
    "        return\n",
    "\n",
    "    # Print table header\n",
    "    print(f\"{'target':<10} | {'source_label':<15} | {'r2_mean':<25} | {'r2_std':<25}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Iterate through the data and print the summary for each experiment\n",
    "    # Sorting ensures a consistent order for comparison\n",
    "    for target_modality in sorted(all_raw_scores.keys()):\n",
    "        sources = all_raw_scores[target_modality]\n",
    "        for source_label in sorted(sources.keys()):\n",
    "            scores = sources[source_label]\n",
    "            if scores:\n",
    "                # Use numpy for mean and std calculation\n",
    "                r2_mean = np.mean(scores)\n",
    "                # ddof=0 for population standard deviation, matching your original np.std\n",
    "                r2_std = np.std(scores, ddof=0)\n",
    "                print(f\"{target_modality:<10} | {source_label:<15} | {r2_mean:<25.10f} | {r2_std:<25.10f}\")\n",
    "            else:\n",
    "                print(f\"{target_modality:<10} | {source_label:<15} | {'N/A':<25} | {'N/A':<25}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    verify_and_print_summary(RAW_SCORES_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "RAW_SCORES_FILE_PATH = \"../../results/downstream/task_00_rsquared/recalculated_r2_scores_RAW.json\"\n",
    "N_REPETITIONS = 10\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA PREPARATION\n",
    "# =============================================================================\n",
    "def prepare_dataframe_for_analysis(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the raw scores and transforms them into a long-format DataFrame suitable\n",
    "    for repeated measures ANOVA and t-tests. It creates a new 'Single' source\n",
    "    type by averaging the single-modality sources for each run.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            all_raw_scores = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Analysis failed: The file '{file_path}' was not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    records = []\n",
    "    # These are the labels for single-modality sources\n",
    "    single_source_labels = ['cna', 'rnaseq', 'rppa', 'wsi']\n",
    "\n",
    "    for target_modality, sources in all_raw_scores.items():\n",
    "        \n",
    "        # --- Create the new 'Single' source type ---\n",
    "        single_source_runs = []\n",
    "        for source_label, scores in sources.items():\n",
    "            # Identify single-modality sources, excluding the one that is the target\n",
    "            if source_label in single_source_labels and source_label != target_modality:\n",
    "                single_source_runs.append(scores)\n",
    "        \n",
    "        # Average across the single-source runs for each of the 10 repetitions\n",
    "        if single_source_runs:\n",
    "            # np.mean(axis=0) calculates the mean down the columns (i.e., for each run)\n",
    "            mean_single_scores = np.mean(single_source_runs, axis=0)\n",
    "            for i in range(N_REPETITIONS):\n",
    "                records.append({\n",
    "                    'run': i,\n",
    "                    'target_modality': target_modality,\n",
    "                    'source_type': 'Single',\n",
    "                    'r2_score': mean_single_scores[i]\n",
    "                })\n",
    "\n",
    "        # --- Add 'Coherent' and 'Multi' source types ---\n",
    "        for source_label, scores in sources.items():\n",
    "            if source_label in ['Coherent', 'Multi']:\n",
    "                for i in range(N_REPETITIONS):\n",
    "                    records.append({\n",
    "                        'run': i,\n",
    "                        'target_modality': target_modality,\n",
    "                        'source_type': source_label,\n",
    "                        'r2_score': scores[i]\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. STATISTICAL ANALYSIS FUNCTIONS\n",
    "# =============================================================================\n",
    "def run_anova(df: pd.DataFrame):\n",
    "    \"\"\"Performs and prints a two-way repeated measures ANOVA.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Two-Way Repeated Measures ANOVA Results\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty, cannot run ANOVA.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        aov = AnovaRM(data=df,\n",
    "                      depvar='r2_score',\n",
    "                      subject='run',\n",
    "                      within=['target_modality', 'source_type'])\n",
    "        res = aov.fit()\n",
    "        print(res.summary())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during ANOVA calculation: {e}\")\n",
    "\n",
    "\n",
    "def run_posthoc_ttests(df: pd.DataFrame):\n",
    "    \"\"\"Performs post-hoc paired t-tests for source_type within each target_modality.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Post-Hoc Paired t-tests for Source Type\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty, cannot run t-tests.\")\n",
    "        return\n",
    "\n",
    "    # Bonferroni correction for 3 comparisons\n",
    "    n_comparisons = 3\n",
    "    alpha = 0.05\n",
    "    corrected_alpha = alpha / n_comparisons\n",
    "    print(f\"Using Bonferroni-corrected alpha = {corrected_alpha:.4f} for significance.\\n\")\n",
    "\n",
    "    for target in df['target_modality'].unique():\n",
    "        print(f\"\\n--- Analysis for Target Modality: {target.upper()} ---\")\n",
    "        target_df = df[df['target_modality'] == target]\n",
    "        \n",
    "        source_types_to_compare = ['Single', 'Coherent', 'Multi']\n",
    "        \n",
    "        # Get the scores for each source type\n",
    "        scores_by_source = {\n",
    "            stype: target_df[target_df['source_type'] == stype]['r2_score'].values\n",
    "            for stype in source_types_to_compare\n",
    "        }\n",
    "\n",
    "        # Perform pairwise t-tests\n",
    "        for s1, s2 in combinations(source_types_to_compare, 2):\n",
    "            scores1 = scores_by_source[s1]\n",
    "            scores2 = scores_by_source[s2]\n",
    "\n",
    "            if len(scores1) == 0 or len(scores2) == 0:\n",
    "                continue\n",
    "\n",
    "            t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
    "            \n",
    "            significance_details = \"\"\n",
    "            if p_value < corrected_alpha:\n",
    "                mean1 = np.mean(scores1)\n",
    "                mean2 = np.mean(scores2)\n",
    "                winner = s1 if mean1 > mean2 else s2\n",
    "                significance_details = f\"Significant ({winner} is better)\"\n",
    "            else:\n",
    "                significance_details = \"Not Significant\"\n",
    "\n",
    "            print(f\"{s1:>10} vs. {s2:<10}: p-value = {p_value:.6f} ({significance_details})\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # Step 1: Prepare the data\n",
    "    analysis_df = prepare_dataframe_for_analysis(RAW_SCORES_FILE_PATH)\n",
    "    \n",
    "    if not analysis_df.empty:\n",
    "        # Step 2: Run the ANOVA\n",
    "        run_anova(analysis_df)\n",
    "        \n",
    "        # Step 3: Run the post-hoc t-tests\n",
    "        run_posthoc_ttests(analysis_df)\n",
    "    else:\n",
    "        print(\"\\nAnalysis could not be performed due to data loading issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a0acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b37ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4a2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
