{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "\n",
    "# THIS FUNCTION IS NOW FIXED\n",
    "def load_stage_labels(cancer: str, labels_dir: str):\n",
    "    \"\"\"\n",
    "    Loads cancer stage labels for a specific cancer type by merging stage and type files.\n",
    "    \"\"\"\n",
    "    # 1. Load both the stage and cancer type files\n",
    "    train_stage_df = pd.read_csv(os.path.join(labels_dir, \"train_stage.csv\"), index_col=0)\n",
    "    train_type_df = pd.read_csv(os.path.join(labels_dir, \"train_cancer_type.csv\"), index_col=0)\n",
    "    \n",
    "    test_stage_df = pd.read_csv(os.path.join(labels_dir, \"test_stage.csv\"), index_col=0)\n",
    "    test_type_df = pd.read_csv(os.path.join(labels_dir, \"test_cancer_type.csv\"), index_col=0)\n",
    "    \n",
    "    # 2. Join them on their index (sample ID)\n",
    "    # This creates a single DataFrame with both 'pathologic_stage' and 'cancertype' columns\n",
    "    train_combined = train_stage_df.join(train_type_df)\n",
    "    test_combined = test_stage_df.join(test_type_df)\n",
    "\n",
    "    # 3. Filter for the specific cancer using the 'cancertype' column\n",
    "    train_labels_all = train_combined[train_combined.cancertype == cancer]\n",
    "    test_labels_all = test_combined[test_combined.cancertype == cancer]\n",
    "\n",
    "    # 4. Select the 'pathologic_stage' column and drop any rows that might be missing a label\n",
    "    train_labels = train_labels_all['stage'].dropna()\n",
    "    test_labels = test_labels_all['stage'].dropna()\n",
    "    \n",
    "    return train_labels, test_labels\n",
    "\n",
    "def load_feature_data_for_classification(data_dir: str):\n",
    "    \"\"\"Loads all feature sets, including the original non-imputed data.\"\"\"\n",
    "    train_data_variants = {\n",
    "        'original': pd.read_csv(os.path.join(data_dir, \"real_data_train.csv\"), index_col=0),\n",
    "        'knn': pd.read_csv(os.path.join(data_dir, f\"imputed_knn.csv\"), index_col=0),\n",
    "        'multi': pd.read_csv(os.path.join(data_dir, f\"imputed_multi.csv\"), index_col=0),\n",
    "        'coherent': pd.read_csv(os.path.join(data_dir, f\"imputed_coherent.csv\"), index_col=0)\n",
    "    }\n",
    "    test_data = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"), index_col=0)\n",
    "    return train_data_variants, test_data\n",
    "\n",
    "def run_classification_pipeline(cancer: str, labels_dir: str, data_dir: str):\n",
    "    \"\"\"Trains and evaluates RF models for a single cancer type.\"\"\"\n",
    "    train_labels, test_labels = load_stage_labels(cancer, labels_dir)\n",
    "    train_data_variants, test_data = load_feature_data_for_classification(data_dir)\n",
    "    train_common_idx = train_labels.index.intersection(train_data_variants['original'].index)\n",
    "    test_common_idx = test_labels.index.intersection(test_data.index)\n",
    "    y_train = train_labels.loc[train_common_idx].sort_index()\n",
    "    y_test = test_labels.loc[test_common_idx].sort_index()\n",
    "    X_test = test_data.loc[test_common_idx].sort_index()\n",
    "    X_train_variants = {}\n",
    "    for name, df in train_data_variants.items():\n",
    "        X_train_variants[name] = df.loc[train_common_idx].sort_index()\n",
    "    if len(y_train) < 20 or len(y_test) < 10 or y_train.nunique() < 2:\n",
    "        return None\n",
    "    results = []\n",
    "    for variant_name, X_train in X_train_variants.items():\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        results.append({\n",
    "            'data_variant': variant_name,\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'macro_f1_score': macro_f1\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# THIS FUNCTION IS NOW FIXED\n",
    "def run_for_all_cancers_classification(labels_dir: str, data_dir: str):\n",
    "    \"\"\"Orchestrates the classification pipeline across all cancer types.\"\"\"\n",
    "    \n",
    "    # 1. Discover all cancer types from the CORRECT file: train_cancer_type.csv\n",
    "    train_type_path = os.path.join(labels_dir, \"train_cancer_type.csv\")\n",
    "    all_cancers = pd.read_csv(train_type_path)['cancertype'].unique()\n",
    "    print(f\"Discovered {len(all_cancers)} cancer types.\")\n",
    "    \n",
    "    all_results_dfs = []\n",
    "    for cancer in sorted(all_cancers):\n",
    "        print(f\"\\n----- Running classification for: {cancer} -----\")\n",
    "        try:\n",
    "            results_df = run_classification_pipeline(cancer, labels_dir, data_dir)\n",
    "            if results_df is not None:\n",
    "                print(f\"Successfully completed analysis for {cancer}.\")\n",
    "                results_df['cancer'] = cancer\n",
    "                all_results_dfs.append(results_df)\n",
    "            else:\n",
    "                 print(f\"Skipping {cancer} due to insufficient data or classes.\")\n",
    "        except Exception as e:\n",
    "            print(f\"!!! An error occurred while processing {cancer}: {e}\")\n",
    "    if all_results_dfs:\n",
    "        final_summary = pd.concat(all_results_dfs, ignore_index=True)\n",
    "        cols = ['cancer', 'data_variant', 'accuracy', 'balanced_accuracy', 'macro_f1_score']\n",
    "        final_summary = final_summary.reindex(columns=cols)\n",
    "        return final_summary\n",
    "    else:\n",
    "        print(\"No results were generated.\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "\n",
    "def load_stage_labels(cancer: str, labels_dir: str):\n",
    "    \"\"\"\n",
    "    Loads cancer stage labels for a specific cancer type by merging stage and type files.\n",
    "    \"\"\"\n",
    "    train_stage_df = pd.read_csv(os.path.join(labels_dir, \"train_stage.csv\"), index_col=0)\n",
    "    train_type_df = pd.read_csv(os.path.join(labels_dir, \"train_cancer_type.csv\"), index_col=0)\n",
    "    test_stage_df = pd.read_csv(os.path.join(labels_dir, \"test_stage.csv\"), index_col=0)\n",
    "    test_type_df = pd.read_csv(os.path.join(labels_dir, \"test_cancer_type.csv\"), index_col=0)\n",
    "    \n",
    "    train_combined = train_stage_df.join(train_type_df)\n",
    "    test_combined = test_stage_df.join(test_type_df)\n",
    "\n",
    "    train_labels_all = train_combined[train_combined.cancertype == cancer]\n",
    "    test_labels_all = test_combined[test_combined.cancertype == cancer]\n",
    "\n",
    "    train_labels = train_labels_all['stage'].dropna()\n",
    "    test_labels = test_labels_all['stage'].dropna()\n",
    "    \n",
    "    return train_labels, test_labels\n",
    "\n",
    "def load_feature_data_for_classification(data_dir: str):\n",
    "    \"\"\"Loads all feature sets, including the original non-imputed data.\"\"\"\n",
    "    train_data_variants = {\n",
    "        'original': pd.read_csv(os.path.join(data_dir, \"real_data_train.csv\"), index_col=0),\n",
    "        'knn': pd.read_csv(os.path.join(data_dir, f\"imputed_knn.csv\"), index_col=0),\n",
    "        'multi': pd.read_csv(os.path.join(data_dir, f\"imputed_multi.csv\"), index_col=0),\n",
    "        'coherent': pd.read_csv(os.path.join(data_dir, f\"imputed_coherent.csv\"), index_col=0)\n",
    "    }\n",
    "    test_data = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"), index_col=0)\n",
    "    return train_data_variants, test_data\n",
    "\n",
    "# MODIFIED FUNCTION\n",
    "def run_classification_pipeline(cancer: str, labels_dir: str, data_dir: str):\n",
    "    \"\"\"Trains and evaluates RF models for a single cancer type.\"\"\"\n",
    "    \n",
    "    train_labels, test_labels = load_stage_labels(cancer, labels_dir)\n",
    "    train_data_variants, test_data = load_feature_data_for_classification(data_dir)\n",
    "\n",
    "    train_common_idx = train_labels.index.intersection(train_data_variants['original'].index)\n",
    "    test_common_idx = test_labels.index.intersection(test_data.index)\n",
    "    \n",
    "    y_train = train_labels.loc[train_common_idx].sort_index()\n",
    "    y_test = test_labels.loc[test_common_idx].sort_index()\n",
    "    \n",
    "    X_test = test_data.loc[test_common_idx].sort_index()\n",
    "\n",
    "    X_train_variants = {}\n",
    "    for name, df in train_data_variants.items():\n",
    "        X_train_variants[name] = df.loc[train_common_idx].sort_index()\n",
    "\n",
    "    if len(y_train) < 20 or len(y_test) < 10 or y_train.nunique() < 2:\n",
    "        return None\n",
    "\n",
    "    # NEW: Capture the train and test sizes after alignment\n",
    "    train_size = len(y_train)\n",
    "    test_size = len(y_test)\n",
    "\n",
    "    results = []\n",
    "    for variant_name, X_train in X_train_variants.items():\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        \n",
    "        # NEW: Add the sizes to the results dictionary\n",
    "        results.append({\n",
    "            'data_variant': variant_name,\n",
    "            'train_size': train_size,\n",
    "            'test_size': test_size,\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'macro_f1_score': macro_f1\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# MODIFIED FUNCTION\n",
    "def run_for_all_cancers_classification(labels_dir: str, data_dir: str):\n",
    "    \"\"\"Orchestrates the classification pipeline across all cancer types.\"\"\"\n",
    "    \n",
    "    train_type_path = os.path.join(labels_dir, \"train_cancer_type.csv\")\n",
    "    all_cancers = pd.read_csv(train_type_path)['cancertype'].unique()\n",
    "    print(f\"Discovered {len(all_cancers)} cancer types.\")\n",
    "    \n",
    "    all_results_dfs = []\n",
    "    for cancer in sorted(all_cancers):\n",
    "        print(f\"\\n----- Running classification for: {cancer} -----\")\n",
    "        try:\n",
    "            results_df = run_classification_pipeline(cancer, labels_dir, data_dir)\n",
    "            if results_df is not None:\n",
    "                print(f\"Successfully completed analysis for {cancer}.\")\n",
    "                results_df['cancer'] = cancer\n",
    "                all_results_dfs.append(results_df)\n",
    "            else:\n",
    "                 print(f\"Skipping {cancer} due to insufficient data or classes.\")\n",
    "        except Exception as e:\n",
    "            print(f\"!!! An error occurred while processing {cancer}: {e}\")\n",
    "    if all_results_dfs:\n",
    "        final_summary = pd.concat(all_results_dfs, ignore_index=True)\n",
    "        \n",
    "        # NEW: Update the column order to include the new size columns\n",
    "        cols = ['cancer', 'data_variant', 'train_size', 'test_size', 'accuracy', 'balanced_accuracy', 'macro_f1_score']\n",
    "        final_summary = final_summary.reindex(columns=cols)\n",
    "        return final_summary\n",
    "    else:\n",
    "        print(\"No results were generated.\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    LABELS_DIR = \"../../datasets_TCGA/downstream_labels\"\n",
    "    DATA_DIR = \"./data_task_02\"\n",
    "    \n",
    "    classification_results = run_for_all_cancers_classification(\n",
    "        labels_dir=LABELS_DIR,\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "\n",
    "    if classification_results is not None:\n",
    "        print(\"\\n\\n===== FINAL SUMMARY OF CLASSIFICATION RESULTS =====\")\n",
    "        sorted_results = classification_results.sort_values(\n",
    "            by=['cancer', 'balanced_accuracy'], ascending=[True, False]\n",
    "        )\n",
    "        print(sorted_results.to_string())\n",
    "        print(\"\\n\\n===== BEST DATA VARIANT PER CANCER (based on Balanced Accuracy) =====\")\n",
    "        best_variants = sorted_results.loc[sorted_results.groupby('cancer')['balanced_accuracy'].idxmax()]\n",
    "        print(best_variants)\n",
    "if __name__ == '__main__':\n",
    "    LABELS_DIR = \"../../datasets_TCGA/downstream_labels\"\n",
    "    DATA_DIR = \"./data_task_02\"\n",
    "    \n",
    "    classification_results = run_for_all_cancers_classification(\n",
    "        labels_dir=LABELS_DIR,\n",
    "        data_dir=DATA_DIR\n",
    "    )\n",
    "\n",
    "    if classification_results is not None:\n",
    "        print(\"\\n\\n===== FINAL SUMMARY OF CLASSIFICATION RESULTS =====\")\n",
    "        sorted_results = classification_results.sort_values(\n",
    "            by=['cancer', 'balanced_accuracy'], ascending=[True, False]\n",
    "        )\n",
    "        print(sorted_results.to_string())\n",
    "        print(\"\\n\\n===== BEST DATA VARIANT PER CANCER (based on Balanced Accuracy) =====\")\n",
    "        best_variants = sorted_results.loc[sorted_results.groupby('cancer')['balanced_accuracy'].idxmax()]\n",
    "        print(best_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import friedmanchisquare, wilcoxon, spearmanr\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from Wilcoxon test on small samples\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='Exact p-value calculation does not work if there are ties. Switching to normal approximation.')\n",
    "\n",
    "def analyze_classification_results(results_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Performs a deep analysis of classification results to compare data variants\n",
    "    and investigate the role of sample size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use balanced_accuracy as our primary performance metric\n",
    "    metric = 'balanced_accuracy'\n",
    "    \n",
    "    print(\"==========================================================\")\n",
    "    print(\" Part 1: Which Data Variant is Better? \")\n",
    "    print(\"==========================================================\")\n",
    "\n",
    "    # --- 1a. Visualization ---\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.boxplot(data=results_df, x='data_variant', y=metric, order=['original', 'knn', 'multi', 'coherent'])\n",
    "    sns.swarmplot(data=results_df, x='data_variant', y=metric, order=['original', 'knn', 'multi', 'coherent'], color='0.25')\n",
    "    plt.title(f'Performance Distribution by Data Variant', fontsize=16)\n",
    "    plt.ylabel(f'Balanced Accuracy Score', fontsize=12)\n",
    "    plt.xlabel('Data Variant', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 1b. Statistical Testing ---\n",
    "    # Reshape data for the Friedman test: each row is a cancer, each column is a data variant\n",
    "    pivot_df = results_df.pivot_table(index='cancer', columns='data_variant', values=metric)\n",
    "    pivot_df.dropna(inplace=True) # Ensure we only test on cancers with results for all variants\n",
    "\n",
    "    print(f\"\\nPerforming statistical tests on {len(pivot_df)} cancer types with complete data.\")\n",
    "\n",
    "    # Friedman Test (checks for any difference among all groups)\n",
    "    stat, p_friedman = friedmanchisquare(pivot_df['original'], pivot_df['knn'], pivot_df['multi'], pivot_df['coherent'])\n",
    "    print(\"\\n--- Friedman Test ---\")\n",
    "    print(f\"Statistic: {stat:.4f}, p-value: {p_friedman:.4f}\")\n",
    "\n",
    "    if p_friedman < 0.05:\n",
    "        print(\"âœ… The test is significant (p < 0.05). There are meaningful differences between the data variants.\")\n",
    "        \n",
    "        # Post-Hoc Pairwise Wilcoxon Tests\n",
    "        print(\"\\n--- Post-Hoc Pairwise Wilcoxon Tests (comparing each pair) ---\")\n",
    "        alpha = 0.05\n",
    "        # 6 pairs to compare: (4*3)/2\n",
    "        num_tests = len(list(combinations(pivot_df.columns, 2)))\n",
    "        corrected_alpha = alpha / num_tests\n",
    "        print(f\"Using Bonferroni corrected alpha = {corrected_alpha:.4f} for significance.\")\n",
    "        \n",
    "        pairs = list(combinations(pivot_df.columns, 2))\n",
    "        for pair in pairs:\n",
    "            method1, method2 = pair\n",
    "            stat_wilcox, p_wilcox = wilcoxon(pivot_df[method1], pivot_df[method2])\n",
    "            \n",
    "            print(f\"\\nComparison: {method1.upper()} vs {method2.upper()}\")\n",
    "            print(f\"  p-value: {p_wilcox:.4f}\")\n",
    "            if p_wilcox < corrected_alpha:\n",
    "                winner = method1 if pivot_df[method1].mean() > pivot_df[method2].mean() else method2\n",
    "                print(f\"  -> Statistically SIGNIFICANT difference found. {winner.upper()} was better on average.\")\n",
    "            else:\n",
    "                print(f\"  -> No significant difference found.\")\n",
    "    else:\n",
    "        print(\"The test is not significant (p >= 0.05). There is no strong evidence of a difference among the data variants.\")\n",
    "\n",
    "    \n",
    "    print(\"\\n\\n==========================================================\")\n",
    "    print(\" Part 2: Does Sample Size Play a Role? \")\n",
    "    print(\"==========================================================\")\n",
    "\n",
    "    # --- 2a. Visualization ---\n",
    "    # Use lmplot to create a scatter plot with a regression line for each data variant\n",
    "    g = sns.lmplot(\n",
    "        data=results_df, x='train_size', y=metric, hue='data_variant',\n",
    "        height=6, aspect=1.5, ci=None, legend_out=False\n",
    "    )\n",
    "    g.fig.suptitle('Role of Training Set Size on Model Performance', y=1.02, fontsize=16)\n",
    "    g.set_axis_labels('Training Set Size', 'Balanced Accuracy Score', fontsize=12)\n",
    "    plt.grid(linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2b. Correlation Analysis ---\n",
    "    print(\"\\n--- Spearman Correlation between Train Size and Performance ---\")\n",
    "    print(\"(Spearman's Rho: +1 is perfect positive correlation, -1 is perfect negative, 0 is no correlation)\\n\")\n",
    "    for variant in results_df['data_variant'].unique():\n",
    "        subset = results_df[results_df['data_variant'] == variant]\n",
    "        corr, p_corr = spearmanr(subset['train_size'], subset[metric])\n",
    "        \n",
    "        print(f\"Variant: {variant.upper()}\")\n",
    "        print(f\"  Spearman's Rho: {corr:.4f}\")\n",
    "        print(f\"  p-value: {p_corr:.4f}\")\n",
    "        if p_corr < 0.05:\n",
    "            print(\"  -> The correlation is statistically significant.\")\n",
    "        else:\n",
    "            print(\"  -> The correlation is not statistically significant.\")\n",
    "\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "analyze_classification_results(classification_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c2de35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
