{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Base path to your “32” folder:\n",
    "base_dir = \"../results/32\"\n",
    "\n",
    "# 2. Prepare an empty list to collect DataFrames:\n",
    "all_best_dfs = []\n",
    "\n",
    "# 3. Walk through each subdirectory of “32”:\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    # Look specifically for grid_search_bst_results.csv\n",
    "    if \"grid_search_bst_results.csv\" in files:\n",
    "        csv_path = os.path.join(root, \"grid_search_bst_results.csv\")\n",
    "        # Derive a human‐readable “dataset_pair” label from the folder structure:\n",
    "        # e.g. root might be \"./results/32/cna_from_multi/train\"\n",
    "        dataset_pair = os.path.basename(os.path.dirname(root))  # “cna_from_multi”\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        df[\"dataset_pair\"] = dataset_pair\n",
    "        all_best_dfs.append(df)\n",
    "\n",
    "# 4. Concatenate all into one DataFrame:\n",
    "if len(all_best_dfs) > 0:\n",
    "    global_best_df = pd.concat(all_best_dfs, ignore_index=True)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No grid_search_bst_results.csv found under ./results/32\")\n",
    "\n",
    "# 5. Show a quick preview:\n",
    "display(global_best_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_df = global_best_df[global_best_df[\"which_best\"] == \"mse\"]\n",
    "display(mse_df.sort_values(\"best_loss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Example: Load the losses of the best MSE model under cna_from_multi ===\n",
    "dataset_name = \"cna_from_multi\"\n",
    "metrics_folder = f\"../results/32/{dataset_name}/train\"\n",
    "\n",
    "# JSON file that was saved when we found a new best MSE:\n",
    "json_path = f\"{metrics_folder}/best_by_mse_losses.json\"\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    losses_dict = json.load(f)\n",
    "    # losses_dict has keys like:\n",
    "    #   \"train_loss\"       : [ epoch0_loss, epoch1_loss, … ]\n",
    "    #   \"val_mse\"          : [ val_mse_at_epoch, … ]\n",
    "    #   \"val_r2\"           : [ … ]\n",
    "    #   \"val_cosine\"       : [ … ]\n",
    "    #   \"val_mse_timestep\" : [ … ]\n",
    "    #   \"val_r2_timestep\"  : [ … ]\n",
    "    #   \"val_cosine_timestep\": [ … ]\n",
    "\n",
    "# Plot train vs. val_mse over epochs (assuming validation was every `validation_epochs`):\n",
    "epochs = list(range(1, len(losses_dict[\"train_loss\"]) + 1))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs, losses_dict[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(epochs, losses_dict[\"val_mse\"],   label=\"val_mse\")\n",
    "plt.plot(epochs, losses_dict[\"val_r2\"],    label=\"val_r2\")\n",
    "plt.plot(epochs, losses_dict[\"val_cosine\"], label=\"val_cosine\")\n",
    "\n",
    "plt.xlabel(\"Val step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"{dataset_name} — Best MSE Model Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810acc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "history_path = \"../results/32/rnaseq_from_multi/train/grid_search_history.json\"\n",
    "\n",
    "with open(history_path, \"r\") as f:\n",
    "    history_list = json.load(f)\n",
    "\n",
    "# Turn it into a flat DataFrame where each row is one experiment\n",
    "rows = []\n",
    "for record in history_list:\n",
    "    entry = {\n",
    "        **record[\"params\"],\n",
    "        **record[\"best_val_losses\"],\n",
    "        \"experiment_index\": record[\"experiment_index\"],\n",
    "    }\n",
    "    rows.append(entry)\n",
    "\n",
    "history_df = pd.DataFrame(rows)\n",
    "display(history_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sort by “best_val_mse” ascending to see the top‐10 runs\n",
    "top10 = history_df.sort_values(\"best_val_mse\").head(10)\n",
    "display(top10)\n",
    "\n",
    "# 2. Check correlation between batch_size and val_mse:\n",
    "display(history_df[[\"batch_size\", \"best_val_mse\"]].corr())\n",
    "\n",
    "# 3. If you want to see epoch‐by‐epoch curves for the very best run:\n",
    "best_row = history_df.sort_values(\"best_val_mse\").iloc[0]\n",
    "best_idx = best_row[\"experiment_index\"]\n",
    "# Find the matching record in history_list to grab losses_history:\n",
    "best_record = next(r for r in history_list if r[\"experiment_index\"] == best_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc7ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5133b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235892c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd15790c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4966cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Base directory containing the “32” folder\n",
    "BASE_DIR = \"../results/32\"\n",
    "\n",
    "# List all dataset‐pair subfolders under BASE_DIR (e.g. “cna_from_multi”, “rnaseq_from_wsi”, etc.)\n",
    "dataset_pairs = [\n",
    "    name for name in os.listdir(BASE_DIR)\n",
    "    if os.path.isdir(os.path.join(BASE_DIR, name))\n",
    "]\n",
    "\n",
    "# For each dataset_pair, we expect a “train” subfolder with the JSONs\n",
    "for ds in dataset_pairs:\n",
    "    train_dir = os.path.join(BASE_DIR, ds, \"train\")\n",
    "    if not os.path.isdir(train_dir):\n",
    "        # Skip if there is no train folder\n",
    "        continue\n",
    "\n",
    "    # Define a helper that loads one of the “best_by_<metric>_losses.json” files\n",
    "    def load_losses(metric_name):\n",
    "        \"\"\"\n",
    "        Loads the losses JSON for best_by_<metric_name>.\n",
    "        Returns a dict with keys:\n",
    "          - \"val_mse\"             : list of validation‐MSE at each validation step\n",
    "          - \"val_cosine\"          : list of validation‐cosine at each validation step\n",
    "          - \"val_mse_timestep\"    : list of validation‐timestep‐MSE at each validation step\n",
    "        \"\"\"\n",
    "        path = os.path.join(train_dir, f\"best_by_{metric_name}_losses.json\")\n",
    "        if not os.path.isfile(path):\n",
    "            raise FileNotFoundError(f\"Could not find {path}\")\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # For each reference metric, we will make one plot\n",
    "    for ref_metric in [\"mse\", \"cosine\", \"timestep\"]:\n",
    "        # 1) Load the JSON containing all loss curves for the model\n",
    "        losses = load_losses(ref_metric)\n",
    "        #    We expect at least these keys:\n",
    "        #       losses[\"val_mse\"]\n",
    "        #       losses[\"val_cosine\"]\n",
    "        #       losses[\"val_mse_timestep\"]\n",
    "        #\n",
    "        #    Each is a list whose length = number of validation checkpoints (e.g. epochs/validation_epochs).\n",
    "\n",
    "        val_mse_list          = losses.get(\"val_mse\", [])\n",
    "        val_cosine_list       = losses.get(\"val_cosine\", [])\n",
    "        val_mse_timestep_list = losses.get(\"val_mse_timestep\", [])\n",
    "\n",
    "        # 2) Determine which index (0‐based) gives the best value of the reference metric:\n",
    "        if ref_metric == \"mse\":\n",
    "            best_idx = min(range(len(val_mse_list)), key=lambda i: val_mse_list[i])\n",
    "            vname = \"Validation MSE\"\n",
    "        elif ref_metric == \"cosine\":\n",
    "            best_idx = min(range(len(val_cosine_list)), key=lambda i: val_cosine_list[i])\n",
    "            vname = \"Validation Cosine\"\n",
    "        else:  # ref_metric == \"timestep\"\n",
    "            best_idx = min(range(len(val_mse_timestep_list)), key=lambda i: val_mse_timestep_list[i])\n",
    "            vname = \"Validation Timestep‐MSE\"\n",
    "\n",
    "        # 3) Build x‐axis as “checkpoint index” (1, 2, 3, …)\n",
    "        steps = list(range(1, len(val_mse_list) + 1))\n",
    "\n",
    "        # 4) Create the plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(steps, val_mse_list,          label=\"val_mse\")\n",
    "        plt.plot(steps, val_cosine_list,       label=\"val_cosine\")\n",
    "        plt.plot(steps, val_mse_timestep_list, label=\"val_mse_timestep\")\n",
    "\n",
    "        # 5) Add a vertical line at the best index + 1 (because our x‐axis is 1‐based)\n",
    "        plt.axvline(x=best_idx + 1, color=\"red\", linestyle=\"--\",\n",
    "                    label=f\"Best {vname} @ step {best_idx + 1}\")\n",
    "\n",
    "        # 6) Labels, title, legend\n",
    "        plt.xlabel(\"Validation Checkpoint Index\")\n",
    "        plt.ylabel(\"Loss / Metric Value\")\n",
    "        plt.title(f\"{ds} — losses for model “best_by_{ref_metric}”\")\n",
    "        plt.legend()\n",
    "\n",
    "        # 7) Optionally save to file or simply show\n",
    "        out_fname = os.path.join(\n",
    "            train_dir,\n",
    "            f\"{ds}_best_by_{ref_metric}_losses_plot.png\"\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_fname, dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved plot for {ds} best_by_{ref_metric} → {out_fname}\")\n",
    "\n",
    "print(\"All plots generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Base directory containing all dataset‐pair folders (e.g. “cna_from_multi”, etc.)\n",
    "BASE_DIR = \"../results/32\"\n",
    "\n",
    "# Output folder for combined images\n",
    "IMAGES_DIR = \"../results/images/32\"\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# List all dataset‐pair subfolders under BASE_DIR\n",
    "dataset_pairs = [\n",
    "    name for name in os.listdir(BASE_DIR)\n",
    "    if os.path.isdir(os.path.join(BASE_DIR, name))\n",
    "]\n",
    "\n",
    "for ds in dataset_pairs:\n",
    "    train_dir = os.path.join(BASE_DIR, ds, \"train\")\n",
    "    if not os.path.isdir(train_dir):\n",
    "        # Skip if there is no \"train\" folder inside this dataset-pair\n",
    "        continue\n",
    "\n",
    "    # Helper to load the losses JSON for a given reference metric\n",
    "    def load_losses(metric_name):\n",
    "        path = os.path.join(train_dir, f\"best_by_{metric_name}_losses.json\")\n",
    "        if not os.path.isfile(path):\n",
    "            raise FileNotFoundError(f\"Could not find {path}\")\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # We will create one figure with 3 subplots (columns):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "    # Keep track of which panel (0,1,2) we are on\n",
    "    for idx, ref_metric in enumerate([\"mse\", \"cosine\", \"timestep\"]):\n",
    "        # Load loss curves for this reference metric\n",
    "        losses = load_losses(ref_metric)\n",
    "        val_mse_list          = losses.get(\"val_mse\", [])\n",
    "        val_cosine_list       = losses.get(\"val_cosine\", [])\n",
    "        val_mse_timestep_list = losses.get(\"val_mse_timestep\", [])\n",
    "\n",
    "        # Determine best‐index for the reference metric\n",
    "        if ref_metric == \"mse\":\n",
    "            best_idx = min(range(len(val_mse_list)), key=lambda i: val_mse_list[i])\n",
    "            panel_title = \"Best by MSE\"\n",
    "        elif ref_metric == \"cosine\":\n",
    "            best_idx = min(range(len(val_cosine_list)), key=lambda i: val_cosine_list[i])\n",
    "            panel_title = \"Best by Cosine\"\n",
    "        else:  # ref_metric == \"timestep\"\n",
    "            best_idx = min(range(len(val_mse_timestep_list)), key=lambda i: val_mse_timestep_list[i])\n",
    "            panel_title = \"Best by Timestep‐MSE\"\n",
    "\n",
    "        # x‐axis = validation checkpoint index (1, 2, 3, …)\n",
    "        steps = list(range(1, len(val_mse_list) + 1))\n",
    "\n",
    "        ax = axes[idx]\n",
    "        ax.plot(steps, val_mse_list,          label=\"val_mse\")\n",
    "        ax.plot(steps, val_cosine_list,       label=\"val_cosine\")\n",
    "        ax.plot(steps, val_mse_timestep_list, label=\"val_mse_timestep\")\n",
    "\n",
    "        # Draw vertical line at the best step (add 1 because steps are 1‐based)\n",
    "        ax.axvline(x=best_idx + 1, color=\"red\", linestyle=\"--\",\n",
    "                   label=f\"Best @ step {best_idx + 1}\")\n",
    "\n",
    "        ax.set_title(panel_title)\n",
    "        ax.set_xlabel(\"Validation Checkpoint\")\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel(\"Loss / Metric Value\")\n",
    "\n",
    "        ax.legend(fontsize=\"small\")\n",
    "\n",
    "    # Overall figure title\n",
    "    fig.suptitle(f\"{ds} — Validation Metrics for best_by_<metric> Models\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Save to ./results/images/<experiment>.png\n",
    "    out_path = os.path.join(IMAGES_DIR, f\"{ds}.png\")\n",
    "    fig.savefig(out_path, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved combined figure for {ds} → {out_path}\")\n",
    "\n",
    "print(\"All combined figures generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7464cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Base directory containing all dataset‐pair folders\n",
    "BASE_DIR = \"../results/32\"\n",
    "\n",
    "# Output directory for images\n",
    "IMAGES_DIR = \"../results/images/32\"\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# List all dataset‐pair names that have a 'train' subfolder\n",
    "dataset_pairs = [\n",
    "    name for name in os.listdir(BASE_DIR)\n",
    "    if os.path.isdir(os.path.join(BASE_DIR, name, \"train\"))\n",
    "]\n",
    "\n",
    "# Number of dataset-pairs\n",
    "num_datasets = len(dataset_pairs)\n",
    "\n",
    "# Create a figure with one row per dataset, three columns (best_by_mse, best_by_cosine, best_by_timestep)\n",
    "fig, axes = plt.subplots(nrows=num_datasets, ncols=3, figsize=(18, 5 * num_datasets), sharey=True)\n",
    "\n",
    "# If there's only one dataset, ensure axes is 2D\n",
    "if num_datasets == 1:\n",
    "    axes = axes.reshape(1, 3)\n",
    "\n",
    "for row_idx, ds in enumerate(dataset_pairs):\n",
    "    train_dir = os.path.join(BASE_DIR, ds, \"train\")\n",
    "    for col_idx, ref_metric in enumerate([\"mse\", \"cosine\", \"timestep\"]):\n",
    "        # Load the corresponding JSON file\n",
    "        json_path = os.path.join(train_dir, f\"best_by_{ref_metric}_losses.json\")\n",
    "        with open(json_path, \"r\") as f:\n",
    "            losses = json.load(f)\n",
    "\n",
    "        val_mse_list = losses.get(\"val_mse\", [])\n",
    "        val_cosine_list = losses.get(\"val_cosine\", [])\n",
    "        val_mse_timestep_list = losses.get(\"val_mse_timestep\", [])\n",
    "\n",
    "        # Determine the best index for the reference metric\n",
    "        if ref_metric == \"mse\":\n",
    "            best_idx = min(range(len(val_mse_list)), key=lambda i: val_mse_list[i])\n",
    "            panel_title = \"Best by MSE\"\n",
    "        elif ref_metric == \"cosine\":\n",
    "            best_idx = min(range(len(val_cosine_list)), key=lambda i: val_cosine_list[i])\n",
    "            panel_title = \"Best by Cosine\"\n",
    "        else:  # \"timestep\"\n",
    "            best_idx = min(range(len(val_mse_timestep_list)), key=lambda i: val_mse_timestep_list[i])\n",
    "            panel_title = \"Best by Timestep‐MSE\"\n",
    "\n",
    "        # X-axis: checkpoint indices (1-based)\n",
    "        steps = list(range(1, len(val_mse_list) + 1))\n",
    "\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        ax.plot(steps, val_mse_list, label=\"val_mse\")\n",
    "        ax.plot(steps, val_cosine_list, label=\"val_cosine\")\n",
    "        ax.plot(steps, val_mse_timestep_list, label=\"val_mse_timestep\")\n",
    "\n",
    "        # Draw vertical line at the best step (add 1 because steps are 1‐based)\n",
    "        ax.axvline(x=best_idx + 1, color=\"red\", linestyle=\"--\", label=f\"Best Step {best_idx + 1}\")\n",
    "\n",
    "        # Fix y-axis range between 0 and 2\n",
    "        ax.set_ylim(0, 2)\n",
    "\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(ds)\n",
    "        ax.set_title(panel_title)\n",
    "        if row_idx == num_datasets - 1:\n",
    "            ax.set_xlabel(\"Checkpoint Index\")\n",
    "\n",
    "        ax.legend(fontsize=\"small\")\n",
    "\n",
    "fig.suptitle(\"All Experiments: Validation Metrics for Best Models\", fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Save combined figure\n",
    "out_path = os.path.join(IMAGES_DIR, \"all_experiments.png\")\n",
    "fig.savefig(out_path, dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Saved combined figure for all experiments → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "path = pathlib.Path(f'../results/32/cna_from_wsi') \n",
    "\n",
    "# read checkpoint dictionary\n",
    "ckpt_path = path / f'train/best_by_mse.pth'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc078d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from checkpoint\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "raw_cfg = ckpt[\"config\"]\n",
    "config = SimpleNamespace(**raw_cfg)\n",
    "state_dict = ckpt[f\"best_model_mse\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt[\"best_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ff853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Base directory containing all dataset‐pair folders\n",
    "BASE_DIR = \"../results/32\"\n",
    "\n",
    "# --- No longer creating plots, so image directories and matplotlib are removed ---\n",
    "\n",
    "# List all dataset‐pair names that have a 'train' subfolder\n",
    "try:\n",
    "    dataset_pairs = [\n",
    "        name for name in os.listdir(BASE_DIR)\n",
    "        if os.path.isdir(os.path.join(BASE_DIR, name, \"train\"))\n",
    "    ]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The base directory '{BASE_DIR}' was not found.\")\n",
    "    exit()\n",
    "\n",
    "# List to store the results\n",
    "epoch_results = []\n",
    "\n",
    "print(f\"Searching for best epoch in {len(dataset_pairs)} experiments based on MSE...\")\n",
    "\n",
    "# Iterate through each experiment to find the best epoch by MSE\n",
    "for experiment_name in dataset_pairs:\n",
    "    train_dir = os.path.join(BASE_DIR, experiment_name, \"train\")\n",
    "    \n",
    "    # We only care about the model selected using the MSE metric\n",
    "    json_path = os.path.join(train_dir, \"best_by_mse_losses.json\")\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"  - WARNING: Skipping '{experiment_name}', file not found: {json_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        losses = json.load(f)\n",
    "\n",
    "    # Get the list of validation MSE values\n",
    "    val_mse_list = losses.get(\"val_mse\", [])\n",
    "\n",
    "    if not val_mse_list:\n",
    "        print(f\"  - WARNING: Skipping '{experiment_name}', no 'val_mse' data in JSON.\")\n",
    "        continue\n",
    "        \n",
    "    # Find the index corresponding to the minimum validation MSE\n",
    "    best_idx = min(range(len(val_mse_list)), key=lambda i: val_mse_list[i])\n",
    "    \n",
    "    # Get the minimum MSE value itself\n",
    "    min_mse_value = val_mse_list[best_idx]\n",
    "\n",
    "    # The \"epoch\" is the checkpoint index, which is 1-based\n",
    "    best_epoch = best_idx + 1\n",
    "\n",
    "    # Store the result\n",
    "    epoch_results.append({\n",
    "        \"Experiment\": experiment_name,\n",
    "        \"Best Epoch (by MSE)\": best_epoch,\n",
    "        \"Min Validation MSE\": f\"{min_mse_value:.6f}\" # Format for readability\n",
    "    })\n",
    "\n",
    "# --- Display the results in a table ---\n",
    "if epoch_results:\n",
    "    df_results = pd.DataFrame(epoch_results)\n",
    "    print(\"\\n--- Best Epoch Results (based on minimum validation MSE) ---\")\n",
    "    print(df_results.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo valid experiment results were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2716df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make plots look nice\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- 1. Analysis of Categorical Hyperparameters (What are the most common winning values?) ---\n",
    "print(\"--- Analysis of Categorical Hyperparameters ---\")\n",
    "\n",
    "# List of categorical columns to analyze\n",
    "categorical_params = ['architecture', 'batch_size', 'learning_rate']\n",
    "\n",
    "for param in categorical_params:\n",
    "    print(f\"\\nValue counts for '{param}':\")\n",
    "    # Print the frequency of each value\n",
    "    print(mse_df[param].value_counts())\n",
    "    \n",
    "    # Create a count plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(y=param, data=mse_df, order=mse_df[param].value_counts().index, palette=\"viridis\")\n",
    "    plt.title(f'Frequency of Best Hyperparameter: {param}')\n",
    "    plt.xlabel('Number of Best Models (by MSE)')\n",
    "    plt.ylabel(param)\n",
    "    plt.show()\n",
    "\n",
    "# --- 2. Analysis of Numerical Hyperparameters (What are their distributions?) ---\n",
    "print(\"\\n--- Analysis of Numerical Hyperparameters ---\")\n",
    "\n",
    "# List of numerical columns to analyze\n",
    "numerical_params = ['n_layers', 'initial_size', 'bottleneck_size', 'time_embedding_dimension', 'cond_embedding_dim']\n",
    "\n",
    "# Get descriptive statistics (mean, std, min, max, etc.)\n",
    "print(\"\\nDescriptive Statistics for Numerical Parameters:\")\n",
    "display(mse_df[numerical_params].describe())\n",
    "\n",
    "# Create histograms to see the distribution of each numerical parameter\n",
    "for param in numerical_params:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(mse_df[param], kde=True, bins=10)\n",
    "    plt.title(f'Distribution of Best Hyperparameter: {param}')\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. Relationship between Hyperparameters and Performance (Loss) ---\n",
    "print(\"\\n--- Relationship between Hyperparameters and Performance ---\")\n",
    "\n",
    "# Box plot for categorical variables vs. best_loss\n",
    "for param in categorical_params:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=param, y='best_loss', data=mse_df, palette=\"coolwarm\")\n",
    "    plt.title(f'Loss Distribution by {param}')\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel('Best MSE Loss')\n",
    "    plt.show()\n",
    "    \n",
    "# Scatter plot for numerical variables vs. best_loss\n",
    "for param in numerical_params:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.scatterplot(x=param, y='best_loss', data=mse_df, alpha=0.7)\n",
    "    plt.title(f'Loss vs. {param}')\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel('Best MSE Loss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- 4. Correlation Analysis (Are certain hyperparameters chosen together?) ---\n",
    "print(\"\\n--- Correlation Analysis of Numerical Hyperparameters ---\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = mse_df[numerical_params].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Best Numerical Hyperparameters')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
